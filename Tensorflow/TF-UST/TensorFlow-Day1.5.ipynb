{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR with Logistic Regression (Binary classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ H(X)=sigmoid(XW)=\\frac{1}{1+e^{-XW}}$\n",
    "### $ cost(W)=-\\frac{1}{m}\\sum{ylog(H(x)) + (1-y)(log(1-H(x)))} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8881937 [[-1.5906348]\n",
      " [-0.7871109]]\n",
      "1000 0.6931636 [[-0.01641939]\n",
      " [-0.01485808]]\n",
      "2000 0.6931472 [[-0.00030838]\n",
      " [-0.00030541]]\n",
      "3000 0.6931472 [[-6.0149696e-06]\n",
      " [-6.0068373e-06]]\n",
      "4000 0.6931472 [[-1.4240860e-07]\n",
      " [-1.4172647e-07]]\n",
      "5000 0.6931472 [[-1.3346786e-07]\n",
      " [-1.3278574e-07]]\n",
      "6000 0.6931472 [[-1.3346786e-07]\n",
      " [-1.3278574e-07]]\n",
      "7000 0.6931472 [[-1.3346786e-07]\n",
      " [-1.3278574e-07]]\n",
      "8000 0.6931472 [[-1.3346786e-07]\n",
      " [-1.3278574e-07]]\n",
      "9000 0.6931472 [[-1.3346786e-07]\n",
      " [-1.3278574e-07]]\n",
      "10000 0.6931472 [[-1.3346786e-07]\n",
      " [-1.3278574e-07]]\n",
      "\n",
      "Hypothesis:  [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct:  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step%1000==0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run(W))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One logistic function unit cannot seperate XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerual Net\n",
    "### 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2,2], name='weight1'))\n",
    "b1 = tf.Variable(tf.random_normal([2], name='bias1'))\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1], name='weight2'))\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2)+b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.77594507 [array([[ 0.7347666 ,  0.66437376],\n",
      "       [-0.02470546,  0.07916614]], dtype=float32), array([[-0.0431173],\n",
      "       [-1.6716932]], dtype=float32)]\n",
      "1000 0.69338965 [array([[0.7161172 , 0.18289429],\n",
      "       [0.05405389, 0.04264087]], dtype=float32), array([[-0.06648836],\n",
      "       [-1.8869497 ]], dtype=float32)]\n",
      "2000 0.6930875 [array([[0.7174946 , 0.03820392],\n",
      "       [0.11313386, 0.02665523]], dtype=float32), array([[-0.09593342],\n",
      "       [-1.8912709 ]], dtype=float32)]\n",
      "3000 0.6927582 [array([[ 0.7373584 , -0.0222228 ],\n",
      "       [ 0.22822186, -0.00304299]], dtype=float32), array([[-0.20983917],\n",
      "       [-1.8883146 ]], dtype=float32)]\n",
      "4000 0.6911157 [array([[ 0.8248022 , -0.11552987],\n",
      "       [ 0.47638288, -0.0793582 ]], dtype=float32), array([[-0.46977276],\n",
      "       [-1.8798715 ]], dtype=float32)]\n",
      "5000 0.6747848 [array([[ 1.1150212 , -0.60414153],\n",
      "       [ 0.9689868 , -0.57034194]], dtype=float32), array([[-1.103569 ],\n",
      "       [-1.9634396]], dtype=float32)]\n",
      "6000 0.42444843 [array([[ 2.1003141, -2.8627975],\n",
      "       [ 2.08672  , -2.8527029]], dtype=float32), array([[-2.9177215],\n",
      "       [-3.8264165]], dtype=float32)]\n",
      "7000 0.14307734 [array([[ 3.3715892, -4.454969 ],\n",
      "       [ 3.3705533, -4.4521694]], dtype=float32), array([[-5.612924 ],\n",
      "       [-6.1774287]], dtype=float32)]\n",
      "8000 0.06879884 [array([[ 3.9890144, -5.104759 ],\n",
      "       [ 3.9886112, -5.1032934]], dtype=float32), array([[-7.1501765],\n",
      "       [-7.4696054]], dtype=float32)]\n",
      "9000 0.04309472 [array([[ 4.3342605, -5.4603357],\n",
      "       [ 4.3340015, -5.4593143]], dtype=float32), array([[-8.079508],\n",
      "       [-8.282462]], dtype=float32)]\n",
      "10000 0.030858409 [array([[ 4.563661, -5.695714],\n",
      "       [ 4.563464, -5.694914]], dtype=float32), array([[-8.728154],\n",
      "       [-8.865076]], dtype=float32)]\n",
      "\n",
      "Hypothesis:  [[0.02468106]\n",
      " [0.96694374]\n",
      " [0.9669462 ]\n",
      " [0.03073329]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step%1000==0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}), sess.run([W1,W2]))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.Variable(tf.random_normal([2,10], name='weight1'))\n",
    "b1 = tf.Variable(tf.random_normal([10], name='bias1'))\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,10], name='weight2'))\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='bias2')\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([10,10], name='weight3'))\n",
    "b3 = tf.Variable(tf.random_normal([10]), name='bias3')\n",
    "layer3 = tf.sigmoid(tf.matmul(layer2,W3)+b3)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([10,1], name='weight4'))\n",
    "b4 = tf.Variable(tf.random_normal([1]), name='bias4')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer3,W4)+b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.72602177\n",
      "100 0.6512417\n",
      "200 0.61226815\n",
      "300 0.56935006\n",
      "400 0.5139572\n",
      "500 0.4377269\n",
      "600 0.33946592\n",
      "700 0.23931846\n",
      "800 0.1605539\n",
      "900 0.10927792\n",
      "1000 0.078076124\n",
      "\n",
      "Hypothesis:  [[0.05227613]\n",
      " [0.90632373]\n",
      " [0.9487957 ]\n",
      " [0.10209578]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step%100==0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal defination\n",
    "### $ W:=W-\\alpha \\frac{\\partial }{\\partial W} \\frac{1}{2m} \\sum _{i=1}^m{(Wx^{(i)}-y^{(i)})^2} $\n",
    "### $ W:=W-\\alpha \\frac{1}{2m} \\sum_{i=1}^m{2(Wx^{(i)}-y^{(i)})x^{(i)}} $\n",
    "### $ W:=W-\\alpha \\frac{1}{m} \\sum_{i=1}^m{(Wx^{(i)}-y^{(i)})x^{(i)}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Gradient descent algorithm\n",
    " ### $ W:=W-\\alpha \\frac{1}{m} \\sum_{i=1}^m{(Wx^{(i)}-y^{(i)})x^{(i)}} $\n",
    " ### $ w = w - \\alpha \\frac{\\partial{E}}{\\partial{W}} $\n",
    " #### train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go deep & wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 9 hidden layers!\n",
    "# 11 Weights\n",
    "W1 = tf.Variable(tf.random_uniform([2,5], -1.0, 1.0), name='Weight1')\n",
    "W2 = tf.Variable(tf.random_uniform([5,5], -1.0, 1.0), name='Weight2')\n",
    "W3 = tf.Variable(tf.random_uniform([5,5], -1.0, 1.0), name='Weight3')\n",
    "W4 = tf.Variable(tf.random_uniform([5,5], -1.0, 1.0), name='Weight4')\n",
    "W5 = tf.Variable(tf.random_uniform([5,5], -1.0, 1.0), name='Weight5')\n",
    "W6 = tf.Variable(tf.random_uniform([5,5], -1.0, 1.0), name='Weight6')\n",
    "W7 = tf.Variable(tf.random_uniform([5,5], -1.0, 1.0), name='Weight7')\n",
    "W8 = tf.Variable(tf.random_uniform([5,5], -1.0, 1.0), name='Weight8')\n",
    "W9 = tf.Variable(tf.random_uniform([5,5], -1.0, 1.0), name='Weight9')\n",
    "W10 = tf.Variable(tf.random_uniform([5,5], -1.0, 1.0), name='Weight10')\n",
    "W11 = tf.Variable(tf.random_uniform([5,1], -1.0, 1.0), name='Weight11')\n",
    "\n",
    "# 11 Bias\n",
    "b1 = tf.Variable(tf.zeros([5], name='Bias1'))\n",
    "b2 = tf.Variable(tf.zeros([5], name='Bias2'))\n",
    "b3 = tf.Variable(tf.zeros([5], name='Bias3'))\n",
    "b4 = tf.Variable(tf.zeros([5], name='Bias4'))\n",
    "b5 = tf.Variable(tf.zeros([5], name='Bias5'))\n",
    "b6 = tf.Variable(tf.zeros([5], name='Bias6'))\n",
    "b7 = tf.Variable(tf.zeros([5], name='Bias7'))\n",
    "b8 = tf.Variable(tf.zeros([5], name='Bias8'))\n",
    "b9 = tf.Variable(tf.zeros([5], name='Bias9'))\n",
    "b10 = tf.Variable(tf.zeros([5], name='Bias10'))\n",
    "b11 = tf.Variable(tf.zeros([1], name='Bias11'))\n",
    "\n",
    "# 11 Layers\n",
    "with tf.name_scope('layer1') as scope:\n",
    "    L1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "with tf.name_scope('layer2') as scope:\n",
    "    L2 = tf.sigmoid(tf.matmul(L1,W2)+b2)\n",
    "with tf.name_scope('layer3') as scope:\n",
    "    L3 = tf.sigmoid(tf.matmul(L2,W3)+b3)\n",
    "with tf.name_scope('layer4') as scope:\n",
    "    L4 = tf.sigmoid(tf.matmul(L3,W4)+b4)\n",
    "with tf.name_scope('layer5') as scope:\n",
    "    L5 = tf.sigmoid(tf.matmul(L4,W5)+b5)\n",
    "with tf.name_scope('layer6') as scope:\n",
    "    L6 = tf.sigmoid(tf.matmul(L5,W6)+b6)\n",
    "with tf.name_scope('layer7') as scope:\n",
    "    L7 = tf.sigmoid(tf.matmul(L6,W7)+b7)\n",
    "with tf.name_scope('layer8') as scope:\n",
    "    L8 = tf.sigmoid(tf.matmul(L7,W8)+b8)\n",
    "with tf.name_scope('layer9') as scope:\n",
    "    L9 = tf.sigmoid(tf.matmul(L8,W9)+b9)\n",
    "with tf.name_scope('layer10') as scope:\n",
    "    L10 = tf.sigmoid(tf.matmul(L9,W10)+b10)\n",
    "with tf.name_scope('last') as scope:\n",
    "    hypothesis = tf.sigmoid(tf.matmul(L10,W11)+b11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.85092825\n",
      "100 0.69314986\n",
      "200 0.6931472\n",
      "300 0.6931472\n",
      "400 0.6931472\n",
      "\n",
      "Hypothesis:  [[0.5000001 ]\n",
      " [0.5000006 ]\n",
      " [0.49999952]\n",
      " [0.5       ]] \n",
      "Correct:  [[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(401):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step%100==0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU: Rectified Linear Unit\n",
    "#### L1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "#### L1 = tf.nn.relu(tf.matmul(X,W1)+b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 11 Layers with ReLU\n",
    "with tf.name_scope('layer1') as scope:\n",
    "    L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "with tf.name_scope('layer2') as scope:\n",
    "    L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "with tf.name_scope('layer3') as scope:\n",
    "    L3 = tf.nn.relu(tf.matmul(L2,W3)+b3)\n",
    "with tf.name_scope('layer4') as scope:\n",
    "    L4 = tf.nn.relu(tf.matmul(L3,W4)+b4)\n",
    "with tf.name_scope('layer5') as scope:\n",
    "    L5 = tf.nn.relu(tf.matmul(L4,W5)+b5)\n",
    "with tf.name_scope('layer6') as scope:\n",
    "    L6 = tf.nn.relu(tf.matmul(L5,W6)+b6)\n",
    "with tf.name_scope('layer7') as scope:\n",
    "    L7 = tf.nn.relu(tf.matmul(L6,W7)+b7)\n",
    "with tf.name_scope('layer8') as scope:\n",
    "    L8 = tf.nn.relu(tf.matmul(L7,W8)+b8)\n",
    "with tf.name_scope('layer9') as scope:\n",
    "    L9 = tf.nn.relu(tf.matmul(L8,W9)+b9)\n",
    "with tf.name_scope('layer10') as scope:\n",
    "    L10 = tf.nn.relu(tf.matmul(L9,W10)+b10)\n",
    "with tf.name_scope('last') as scope:\n",
    "    hypothesis = tf.sigmoid(tf.matmul(L10,W11)+b11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6848593\n",
      "1000 0.023016274\n",
      "2000 0.010894345\n",
      "3000 0.007102621\n",
      "4000 0.0052616266\n",
      "5000 0.0041755983\n",
      "6000 0.0034604608\n",
      "7000 0.0029537505\n",
      "8000 0.002576195\n",
      "9000 0.002284031\n",
      "10000 0.0020512838\n",
      "\n",
      "Hypothesis:  [[0.00408929]\n",
      " [0.99999166]\n",
      " [0.99999857]\n",
      " [0.00408929]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else false\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step%1000==0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}))\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Works very well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerual Network tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initializing weights\n",
    "** W = tf.Variable(tf.random_normal([1]), name='weight') **\n",
    "\n",
    "** W = tf.get_variable(\"W\", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer()) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Activation functions\n",
    "** tf.sigmoid **\n",
    "\n",
    "** tf.tanh **\n",
    "\n",
    "** tf.nn.relu **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regularization\n",
    "#### Solutions for overfitting\n",
    "* More training data\n",
    "* Reduce the number of features\n",
    "* Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $ L(W)=\\frac{1}{N}\\sum_{i=1}^N{L_i(f(x_i,W),y_i})+\\lambda R(W) $\n",
    "#### $ L=\\frac{1}{N}\\sum_{i=1}^N \\sum_{j\\neq y_i} max \\left[ 0,f(x_i;W)_j - f(x_i;W)_{y_i}+1 \\right] + \\lambda R(W) $\n",
    "#### Î» =  regularization strength (hyperparameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In common use:\n",
    "#### L2 regularization   $ R(W)=\\sum_k \\sum_l W_{k,l}^2 $\n",
    "#### L1 regularization   $ R(W)=\\sum_k \\sum_l \\left|W_{k,l}\\right| $\n",
    "#### Elastic net (L1+L2)    $ R(W)=\\sum_k \\sum_l \\beta W_{k,l}^2 + \\left|W_{k.l}\\right| $\n",
    "#### Max norm rgularization\n",
    "#### Dropout\n",
    "#### Fancier: Batch normalization, stochastic depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable Weight1_6/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-eabcd8071922>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# define cost/loss & optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[1;32m--> 369\u001b[1;33m                                 name=name)\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    518\u001b[0m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[0;32m    519\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_get_variable_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;31m# Create slots for the first and second moments.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"m\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"v\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[1;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m       \u001b[0mnamed_slots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[1;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[0;32m    172\u001b[0m     return create_slot_with_initializer(\n\u001b[0;32m    173\u001b[0m         \u001b[0mprimary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslot_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m         colocate_with_primary=colocate_with_primary)\n\u001b[0m\u001b[0;32m    175\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[1;34m(primary, initializer, shape, dtype, name, colocate_with_primary)\u001b[0m\n\u001b[0;32m    146\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n\u001b[1;32m--> 148\u001b[1;33m                                 dtype)\n\u001b[0m\u001b[0;32m    149\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m       return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[1;34m(primary, val, scope, validate_shape, shape, dtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_is_resource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       validate_shape=validate_shape)\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1295\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1298\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1299\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1091\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1093\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    437\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    406\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    763\u001b[0m       raise ValueError(\"Variable %s does not exist, or was not created with \"\n\u001b[0;32m    764\u001b[0m                        \u001b[1;34m\"tf.get_variable(). Did you mean to set \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m                        \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\n\u001b[0m\u001b[0;32m    766\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n",
      "\u001b[1;31mValueError\u001b[0m: Variable Weight1_6/Adam_2/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?"
     ]
    }
   ],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "    # dropout (keep_prob) rate 0.7 on training, but should be 1 for testing\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, 784])\n",
    "    Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "    W1 = tf.get_variable(\"W1\", shape=[784,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([512]))\n",
    "    L1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "    W2 = tf.get_variable(\"W2\", shape=[512,512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([512]))\n",
    "    L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "    # parameters\n",
    "    learning_rate = 0.001\n",
    "    training_epochs = 15\n",
    "    batch_size = 100\n",
    "    total_batch = int(mnist.test.num_examples / batch_size)\n",
    "\n",
    "    # define cost/loss & optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys, keep_prob:0.7}\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += c/total_batch\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correction_prediction, tf.float32))\n",
    "    print('Accuracy:', sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels, keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-2-83ca7ab530d4>:61: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch: 0001 cost = 0.451240691\n",
      "Epoch: 0002 cost = 0.174481454\n",
      "Epoch: 0003 cost = 0.128587236\n",
      "Epoch: 0004 cost = 0.109119285\n",
      "Epoch: 0005 cost = 0.092670795\n",
      "Epoch: 0006 cost = 0.082088990\n",
      "Epoch: 0007 cost = 0.077748027\n",
      "Epoch: 0008 cost = 0.070568659\n",
      "Epoch: 0009 cost = 0.063686291\n",
      "Epoch: 0010 cost = 0.057611010\n",
      "Epoch: 0011 cost = 0.062052387\n",
      "Epoch: 0012 cost = 0.052562773\n",
      "Epoch: 0013 cost = 0.051294356\n",
      "Epoch: 0014 cost = 0.046317724\n",
      "Epoch: 0015 cost = 0.047256546\n",
      "Learning Finished!\n",
      "Accuracy: 0.9812\n",
      "Label:  [1]\n",
      "Prediction:  [1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch: 0001 cost = 0.447322626\\nEpoch: 0002 cost = 0.157285590\\nEpoch: 0003 cost = 0.121884535\\nEpoch: 0004 cost = 0.098128681\\nEpoch: 0005 cost = 0.082901778\\nEpoch: 0006 cost = 0.075337573\\nEpoch: 0007 cost = 0.069752543\\nEpoch: 0008 cost = 0.060884363\\nEpoch: 0009 cost = 0.055276413\\nEpoch: 0010 cost = 0.054631256\\nEpoch: 0011 cost = 0.049675195\\nEpoch: 0012 cost = 0.049125314\\nEpoch: 0013 cost = 0.047231930\\nEpoch: 0014 cost = 0.041290121\\nEpoch: 0015 cost = 0.043621063\\nLearning Finished!\\nAccuracy: 0.9804\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 10 MNIST and Dropout\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "'''\n",
    "Epoch: 0001 cost = 0.447322626\n",
    "Epoch: 0002 cost = 0.157285590\n",
    "Epoch: 0003 cost = 0.121884535\n",
    "Epoch: 0004 cost = 0.098128681\n",
    "Epoch: 0005 cost = 0.082901778\n",
    "Epoch: 0006 cost = 0.075337573\n",
    "Epoch: 0007 cost = 0.069752543\n",
    "Epoch: 0008 cost = 0.060884363\n",
    "Epoch: 0009 cost = 0.055276413\n",
    "Epoch: 0010 cost = 0.054631256\n",
    "Epoch: 0011 cost = 0.049675195\n",
    "Epoch: 0012 cost = 0.049125314\n",
    "Epoch: 0013 cost = 0.047231930\n",
    "Epoch: 0014 cost = 0.041290121\n",
    "Epoch: 0015 cost = 0.043621063\n",
    "Learning Finished!\n",
    "Accuracy: 0.9804\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Optimizers\n",
    "** tf.train.GradientDescentOptimizer **\n",
    "\n",
    "** tf.train.AdadeltaOptimizer **\n",
    "\n",
    "** tf.train.AdagradOptimizer **\n",
    "\n",
    "** tf.train.AdagradDAOptimizer **\n",
    "\n",
    "** tf.train.MomentumOptimer **\n",
    "\n",
    "** tf.train.AdamOptimizer **\n",
    "\n",
    "** tf.train.FtrlOptimer **\n",
    "\n",
    "** tf.train.ProximalGradientDescentOptimizer **\n",
    "\n",
    "** tf.train.ProxiamlAdagradOptimizer **\n",
    "\n",
    "** tf.train.RMSPropOptimizer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. From TF graph, decide which tensors you want to log\n",
    "w2_hist = tf.summary.histogram(\"weight2\", w2) <br> cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "### 2. Merge all summaries\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "### 3. Create writer and add graph\n",
    "writer =tf.summary.FileWriter('./logs') <br>\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "### 4. Run summary merge and add_summary\n",
    "s, _ = sess.run([summary, optimizer], feed_dict=feed_dict) <br>\n",
    "writer.add_summary(s, global_step=global_step)\n",
    "\n",
    "### 5. Launch TensorBoard\n",
    "tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scalar tensors\n",
    "cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "\n",
    "# Histogram(muti-dimensional tensors)\n",
    "W2 = tf.Variable(tf.random_normal([2,1]), name='weight2')\n",
    "b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "W2_hist = tf.summary.histogram('weight2', W2)\n",
    "b2_hist = tf.summary.histogram('bias2', b2)\n",
    "hypothesis_hist = tf.summary.histogram('hypothesis', hypothesis)\n",
    "\n",
    "# Add scope for better graph hierarchy\n",
    "with tf.name_scope('layer1') as scope:\n",
    "    W1 = tf.Variable(tf.random_normal([2,2]), name='weight1')\n",
    "    b1 = tf.Variable(tf.random_normal([2]), name='bias1')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "    \n",
    "    W1_hist = tf.summary.histogram('weight1', W1)\n",
    "    b1_hist = tf.summary.histogram('bias1', b1)\n",
    "    layer1 = tf.summary.histogram('layer1', layer1)\n",
    "    \n",
    "with tf.name_scope('layer2') as scope:\n",
    "    W2 = tf.Variable(tf.random_normal([2,1]), name='weight2')\n",
    "    b2 = tf.Variable(tf.random_normal([1]), name='bias2')\n",
    "    hypothesis = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "    \n",
    "    W2_hist = tf.summary.histogram('weight2', W2)\n",
    "    b2_hist = tf.summary.histogram('bias2', b2)\n",
    "    hypothesis_hist = tf.summary.histogram('hypothesis', hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2/3. Merge summaries and create writer after creating session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summaray\n",
    "summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Crate summary writer\n",
    "writer = tf.summary.FileWriter(TB_SUMMARY_DIR)\n",
    "writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run merged summary and write (add summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s, _ = sess.run([summary, optimizer], feed_dict=feed_dict)\n",
    "writer.add_summary(s, global_step=global_step)\n",
    "global_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Launch tensorboard\n",
    "#### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./logs/xor_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ tensorboard -logdir=./logs/xor_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remote server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ ssh -L local_port:127.0.0.1:remote_port username@server.com\n",
    "\n",
    "$ tensorboard -logdir=./logs/xor_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P1 ç©éµç¸ä¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('graph') as scope:\n",
    "    matrix1 = tf.constant([[3., 3.]],name ='matrix1')  #1 row by 2 column\n",
    "    matrix2 = tf.constant([[2.],[2.]],name ='matrix2') # 2 row by 1 column\n",
    "    product = tf.matmul(matrix1, matrix2, name='product')\n",
    "\n",
    "sess = tf.Session()\n",
    "writer = tf.summary.FileWriter(\"logs/matmul\", sess.graph)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P2 çº¿æ§æå1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 weight: [-0.0441124] bias: [0.37764052]\n",
      "10 weight: [0.10195768] bias: [0.20252737]\n",
      "20 weight: [0.19805713] bias: [0.15277623]\n",
      "30 weight: [0.24752462] bias: [0.1271667]\n",
      "40 weight: [0.27298817] bias: [0.11398415]\n",
      "50 weight: [0.28609556] bias: [0.10719839]\n",
      "60 weight: [0.29284266] bias: [0.1037054]\n",
      "70 weight: [0.29631573] bias: [0.10190737]\n",
      "80 weight: [0.29810348] bias: [0.10098183]\n",
      "90 weight: [0.29902375] bias: [0.10050541]\n",
      "100 weight: [0.2994975] bias: [0.10026016]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the original data\n",
    "with tf.name_scope('data'):\n",
    "    x_data = np.random.rand(100).astype(np.float32)\n",
    "    y_data = 0.3 * x_data + 0.1\n",
    "\n",
    "# Create parameters\n",
    "with tf.name_scope('paramters'):\n",
    "    with tf.name_scope('weights'):\n",
    "        weight = tf.Variable(tf.random_uniform([1],-1.0,1.0))\n",
    "        tf.summary.histogram('weight', weight)\n",
    "    with tf.name_scope('biases'):\n",
    "        bias = tf.Variable(tf.zeros([1]))\n",
    "        tf.summary.histogram('bias', bias)\n",
    "\n",
    "# Get y_prediction\n",
    "with tf.name_scope('y_prediction'):\n",
    "    y_prediction = weight * x_data + bias\n",
    "    \n",
    "# Compute the loss\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.square(y_data - y_prediction))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "# Create optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "\n",
    "# Create train, minimize the loss\n",
    "with tf.name_scope('train'):\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "# Create init\n",
    "with tf.name_scope('init'):\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "# Create a session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Merged\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Initialize\n",
    "writer = tf.summary.FileWriter(\"logs/lr1\", sess.graph)\n",
    "sess.run(init)\n",
    "\n",
    "# Loop\n",
    "for step in range(101):\n",
    "    sess.run(train)\n",
    "    # rs = sess.run(merged)\n",
    "    # writer.add_summary(rs, step)\n",
    "    if step%10 == 0:\n",
    "        print(step, 'weight:', sess.run(weight), 'bias:', sess.run(bias))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
