{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Hello World ！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0)\n",
    "node3 = tf.add(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_1:0\", shape=(), dtype=float32) node2: Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"node1:\", node1, \"node2:\", node2)\n",
    "print(\"node3:\", node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1, node2):  [3.0, 4.0]\n",
      "sess.run(node3) 7.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(\"sess.run(node1, node2): \", sess.run([node1, node2]))\n",
    "print(\"sess.run(node3)\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[3. 7.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a: [1,3], b:[2,4]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything is Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp: [[[1.0, 2.0, 3.0]], [[7.0, 8.0, 9.0]]]\n",
      "temp[0]: [[1.0, 2.0, 3.0]]\n",
      "temp[0][0]: [1.0, 2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "[1., 2., 3.]\n",
    "[[1., 2., 3.], [4., 5., 6.]]\n",
    "temp = [[[1., 2., 3.]], [[7., 8., 9.]]]\n",
    "print(\"temp:\", temp)\n",
    "print(\"temp[0]:\", temp[0])\n",
    "print(\"temp[0][0]:\", temp[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "t = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**———————————————————————————————————————**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ cost=\\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)}-y^{(i)}))^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ H(x)=Wx+b $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cost(W,b)=\\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)}-y^{(i)}))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Hypothesis in TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(x)=Wx$  \n",
    "y_model = tf.mul(x, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cost(W)=\\frac{1}{m}\\sum_{i=1}^{m}(Wx^{(i)}-y^{(i)})^2$\n",
    "\n",
    "cost = tf.square(Y - y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Minimize Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\min_{W,b} cost(W,b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cost(W)=\\frac{1}{2m}\\sum_{i=1}^{m}(Wx^{(i)}-y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W:=W-\\alpha \\frac{\\partial }{\\partial W}cost(W) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm in TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W:=W-\\alpha \\frac{\\partial }{\\partial W}cost(W) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W:=W-\\alpha\\frac{1}{m}\\sum_{i=1}^{m}(Wx^{(i)}-y^{(i)})x^{(i)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost = tf.square(Y - y_model) <br>\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build graph using TF operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ H(x)=Wx+b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = X_train * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cost(W,b)=\\frac{1}{m}\\sum_{i=1}^{m}(H(x^{(i)}-y^{(i)}))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run/update graph and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.4345753 [-0.10596581] [0.59355736]\n",
      "20 0.13677426 [0.55321497] [0.8327728]\n",
      "40 0.09717847 [0.6320298] [0.8190695]\n",
      "60 0.08801385 [0.65482837] [0.782998]\n",
      "80 0.07993335 [0.6715744] [0.7464306]\n",
      "100 0.07259675 [0.6870591] [0.71137303]\n",
      "120 0.06593353 [0.701771] [0.67794317]\n",
      "140 0.059881896 [0.7157872] [0.6460826]\n",
      "160 0.054385662 [0.72914416] [0.6157189]\n",
      "180 0.049393896 [0.7418735] [0.5867823]\n",
      "200 0.044860348 [0.7540044] [0.5592056]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    sess.run(train)\n",
    "    if step%20 == 0:\n",
    "        print(step, sess.run(cost), sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.044860348 [0.75459576] [0.5578613]\n",
      "20 0.040742885 [0.76612896] [0.53164405]\n",
      "40 0.03700335 [0.77712] [0.5066588]\n",
      "60 0.033607055 [0.78759444] [0.4828477]\n",
      "80 0.030522449 [0.7975768] [0.46015564]\n",
      "100 0.027720988 [0.8070899] [0.43853]\n",
      "120 0.025176628 [0.81615597] [0.41792065]\n",
      "140 0.022865811 [0.82479596] [0.3982799]\n",
      "160 0.020767095 [0.8330299] [0.37956223]\n",
      "180 0.018861016 [0.84087694] [0.3617242]\n",
      "200 0.017129874 [0.8483551] [0.34472448]\n"
     ]
    }
   ],
   "source": [
    "for step in range(201):\n",
    "    cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],\n",
    "                                         feed_dict={X:[1,2,3], Y:[1,2,3]})\n",
    "    if step%20 == 0:\n",
    "        print(step, cost_val, W_val, b_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**———————————————————————————————————————**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Binaray Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ H(X)=sigmoid(XW)=\\frac{1}{1+e^{-XW}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ cost(W)=-\\frac{1}{m}\\sum{ylog(H(x)) + (1-y)(log(1-H(x)))} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $W:=W-\\alpha \\frac{\\partial }{\\partial W}cost(W) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification in TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ H(X)=sigmoid(XW)=\\frac{1}{1+e^{-XW}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ cost(W)=-\\frac{1}{m}\\sum{ylog(H(x)) + (1-y)(log(1-H(x)))} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost = -tf.reduce_mean(Y \\* tf.log(hypothesis) + (1-Y) \\* tf.log(1-hypothesis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7313166\n",
      "200 0.6265866\n",
      "400 0.571963\n",
      "600 0.53763884\n",
      "800 0.5120407\n",
      "1000 0.49076414\n",
      "1200 0.4719453\n",
      "1400 0.45472428\n",
      "1600 0.4386749\n",
      "1800 0.42357126\n",
      "2000 0.40928447\n",
      "2200 0.3957338\n",
      "2400 0.3828636\n",
      "2600 0.3706307\n",
      "2800 0.35899922\n",
      "3000 0.34793666\n",
      "3200 0.3374131\n",
      "3400 0.32740003\n",
      "3600 0.3178701\n",
      "3800 0.3087977\n",
      "4000 0.30015728\n",
      "4200 0.29192516\n",
      "4400 0.2840785\n",
      "4600 0.27659577\n",
      "4800 0.2694563\n",
      "5000 0.26264074\n",
      "5200 0.25613067\n",
      "5400 0.24990888\n",
      "5600 0.24395907\n",
      "5800 0.23826592\n",
      "6000 0.23281507\n",
      "6200 0.22759295\n",
      "6400 0.22258694\n",
      "6600 0.21778528\n",
      "6800 0.21317667\n",
      "7000 0.2087508\n",
      "7200 0.2044977\n",
      "7400 0.20040841\n",
      "7600 0.19647431\n",
      "7800 0.1926871\n",
      "8000 0.18903942\n",
      "8200 0.1855241\n",
      "8400 0.18213458\n",
      "8600 0.1788644\n",
      "8800 0.17570788\n",
      "9000 0.17265941\n",
      "9200 0.16971368\n",
      "9400 0.16686605\n",
      "9600 0.16411161\n",
      "9800 0.16144626\n",
      "10000 0.15886578\n",
      "\n",
      "Hypothesis:  [[0.03477809]\n",
      " [0.1639826 ]\n",
      " [0.323639  ]\n",
      " [0.77293295]\n",
      " [0.93412584]\n",
      " [0.97835755]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step%200 == 0:\n",
    "            print(step, cost_val)\n",
    "    \n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 9)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "xy = np.loadtxt('./Data/data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "print(np.shape(xy))\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.9922662\n",
      "200 1.1346916\n",
      "400 1.0966324\n",
      "600 1.0926877\n",
      "800 1.0901899\n",
      "1000 1.0880598\n",
      "1200 1.0862182\n",
      "1400 1.0846245\n",
      "1600 1.0832442\n",
      "1800 1.0820475\n",
      "2000 1.0810084\n",
      "2200 1.0801052\n",
      "2400 1.0793189\n",
      "2600 1.0786327\n",
      "2800 1.0780331\n",
      "3000 1.0775079\n",
      "3200 1.0770468\n",
      "3400 1.0766408\n",
      "3600 1.076283\n",
      "3800 1.0759662\n",
      "4000 1.0756853\n",
      "4200 1.0754354\n",
      "4400 1.0752122\n",
      "4600 1.0750126\n",
      "4800 1.0748334\n",
      "5000 1.0746719\n",
      "5200 1.0745262\n",
      "5400 1.0743941\n",
      "5600 1.0742738\n",
      "5800 1.0741645\n",
      "6000 1.0740643\n",
      "6200 1.0739725\n",
      "6400 1.073888\n",
      "6600 1.0738102\n",
      "6800 1.0737382\n",
      "7000 1.0736716\n",
      "7200 1.0736095\n",
      "7400 1.0735518\n",
      "7600 1.073498\n",
      "7800 1.0734477\n",
      "8000 1.0734006\n",
      "8200 1.0733565\n",
      "8400 1.0733149\n",
      "8600 1.0732757\n",
      "8800 1.0732391\n",
      "9000 1.0732043\n",
      "9200 1.0731714\n",
      "9400 1.0731403\n",
      "9600 1.0731109\n",
      "9800 1.0730832\n",
      "10000 1.0730567\n",
      "\n",
      "Hypothesis:  [[0.27976295]\n",
      " [0.53462493]\n",
      " [0.22706199]\n",
      " [0.5492545 ]\n",
      " [0.20918292]\n",
      " [0.42391244]\n",
      " [0.5539782 ]\n",
      " [0.36287406]\n",
      " [0.21389909]\n",
      " [0.3358799 ]\n",
      " [0.37939054]\n",
      " [0.2014946 ]\n",
      " [0.2677473 ]\n",
      " [0.22190197]\n",
      " [0.4299238 ]\n",
      " [0.29354677]\n",
      " [0.41669065]\n",
      " [0.44413728]\n",
      " [0.41027832]\n",
      " [0.31840375]\n",
      " [0.39735743]\n",
      " [0.15537179]\n",
      " [0.37182805]\n",
      " [0.36788902]\n",
      " [0.26630306]\n",
      " [0.5237449 ]\n",
      " [0.33410484]\n",
      " [0.36454147]\n",
      " [0.38407716]\n",
      " [0.2694049 ]\n",
      " [0.5537264 ]\n",
      " [0.50908107]\n",
      " [0.35630223]\n",
      " [0.45686236]\n",
      " [0.28565833]\n",
      " [0.38539004]\n",
      " [0.44253412]\n",
      " [0.34275782]\n",
      " [0.24829069]\n",
      " [0.27199274]\n",
      " [0.45358333]\n",
      " [0.16954753]\n",
      " [0.28904834]\n",
      " [0.1343106 ]\n",
      " [0.33102104]\n",
      " [0.5465693 ]\n",
      " [0.38422185]\n",
      " [0.40599424]\n",
      " [0.52050704]\n",
      " [0.52014506]\n",
      " [0.53739554]\n",
      " [0.21385024]\n",
      " [0.26580226]\n",
      " [0.60984665]\n",
      " [0.19573559]\n",
      " [0.31750607]\n",
      " [0.19444065]\n",
      " [0.3808029 ]\n",
      " [0.5035681 ]\n",
      " [0.3245018 ]\n",
      " [0.62742734]\n",
      " [0.36737823]\n",
      " [0.3872767 ]\n",
      " [0.46152213]\n",
      " [0.35308942]\n",
      " [0.33949476]\n",
      " [0.5631748 ]\n",
      " [0.34254697]\n",
      " [0.44162154]\n",
      " [0.35375226]\n",
      " [0.2521915 ]\n",
      " [0.3496142 ]\n",
      " [0.51566756]\n",
      " [0.5221034 ]\n",
      " [0.5290376 ]\n",
      " [0.41974005]\n",
      " [0.2858583 ]\n",
      " [0.45738572]\n",
      " [0.48464054]\n",
      " [0.5388704 ]\n",
      " [0.48286727]\n",
      " [0.458179  ]\n",
      " [0.24839947]\n",
      " [0.41598985]\n",
      " [0.36044854]\n",
      " [0.44318062]\n",
      " [0.28887963]\n",
      " [0.47814617]\n",
      " [0.5992647 ]\n",
      " [0.38073733]\n",
      " [0.43088648]\n",
      " [0.38909563]\n",
      " [0.36318398]\n",
      " [0.3147471 ]\n",
      " [0.49595514]\n",
      " [0.63588387]\n",
      " [0.49777654]\n",
      " [0.31144804]\n",
      " [0.2172948 ]\n",
      " [0.35019854]\n",
      " [0.38239652]\n",
      " [0.5684746 ]\n",
      " [0.42685816]\n",
      " [0.40075585]\n",
      " [0.49501643]\n",
      " [0.35394058]\n",
      " [0.51661855]\n",
      " [0.42282173]\n",
      " [0.26745346]\n",
      " [0.25787959]\n",
      " [0.52181095]\n",
      " [0.5025101 ]\n",
      " [0.2780702 ]\n",
      " [0.29926112]\n",
      " [0.3610716 ]\n",
      " [0.48500323]\n",
      " [0.46992412]\n",
      " [0.5212652 ]\n",
      " [0.17189443]\n",
      " [0.39340335]\n",
      " [0.43579653]\n",
      " [0.37754893]\n",
      " [0.36913612]\n",
      " [0.41006225]\n",
      " [0.34739223]\n",
      " [0.42668203]\n",
      " [0.40204528]\n",
      " [0.39665526]\n",
      " [0.26848555]\n",
      " [0.31124362]\n",
      " [0.25827128]\n",
      " [0.42924228]\n",
      " [0.5275583 ]\n",
      " [0.4082045 ]\n",
      " [0.43432927]\n",
      " [0.4665204 ]\n",
      " [0.31247398]\n",
      " [0.41219607]\n",
      " [0.4214912 ]\n",
      " [0.3810481 ]\n",
      " [0.45797214]\n",
      " [0.3891487 ]\n",
      " [0.3028209 ]\n",
      " [0.39379764]\n",
      " [0.549201  ]\n",
      " [0.40439785]\n",
      " [0.30415648]\n",
      " [0.5230593 ]\n",
      " [0.33250904]\n",
      " [0.44748226]\n",
      " [0.226185  ]\n",
      " [0.24774723]\n",
      " [0.156822  ]\n",
      " [0.20802559]\n",
      " [0.5132218 ]\n",
      " [0.46274632]\n",
      " [0.5303723 ]\n",
      " [0.18076308]\n",
      " [0.30250686]\n",
      " [0.41865748]\n",
      " [0.33007276]\n",
      " [0.4733936 ]\n",
      " [0.30036783]\n",
      " [0.4250883 ]\n",
      " [0.31568056]\n",
      " [0.37462676]\n",
      " [0.40538192]\n",
      " [0.43527764]\n",
      " [0.43203703]\n",
      " [0.3243004 ]\n",
      " [0.48937604]\n",
      " [0.46922156]\n",
      " [0.5649984 ]\n",
      " [0.2105395 ]\n",
      " [0.47309083]\n",
      " [0.19319336]\n",
      " [0.26553094]\n",
      " [0.28196368]\n",
      " [0.52119863]\n",
      " [0.34920517]\n",
      " [0.51197237]\n",
      " [0.55067337]\n",
      " [0.34804788]\n",
      " [0.16880055]\n",
      " [0.21134152]\n",
      " [0.3294464 ]\n",
      " [0.4093039 ]\n",
      " [0.333263  ]\n",
      " [0.4801476 ]\n",
      " [0.35879257]\n",
      " [0.2710559 ]\n",
      " [0.22022204]\n",
      " [0.5276168 ]\n",
      " [0.24794687]\n",
      " [0.5009969 ]\n",
      " [0.48039436]\n",
      " [0.3890104 ]\n",
      " [0.33225307]\n",
      " [0.35677856]\n",
      " [0.30968988]\n",
      " [0.39501086]\n",
      " [0.5444983 ]\n",
      " [0.4120709 ]\n",
      " [0.439343  ]\n",
      " [0.16601186]\n",
      " [0.2413315 ]\n",
      " [0.4963233 ]\n",
      " [0.19548315]\n",
      " [0.5419845 ]\n",
      " [0.2342522 ]\n",
      " [0.21573423]\n",
      " [0.27290773]\n",
      " [0.38109422]\n",
      " [0.21604805]\n",
      " [0.39981008]\n",
      " [0.37922242]\n",
      " [0.44615775]\n",
      " [0.37969467]\n",
      " [0.18880321]\n",
      " [0.26699802]\n",
      " [0.41399652]\n",
      " [0.32688078]\n",
      " [0.5079753 ]\n",
      " [0.51592696]\n",
      " [0.39697656]\n",
      " [0.24901113]\n",
      " [0.11573985]\n",
      " [0.33285585]\n",
      " [0.2484301 ]\n",
      " [0.27834672]\n",
      " [0.5576215 ]\n",
      " [0.35940388]\n",
      " [0.5621167 ]\n",
      " [0.20155795]\n",
      " [0.15908258]\n",
      " [0.22550118]\n",
      " [0.45218876]\n",
      " [0.51889485]\n",
      " [0.47481146]\n",
      " [0.36660546]\n",
      " [0.37200898]\n",
      " [0.3037114 ]\n",
      " [0.1778519 ]\n",
      " [0.35471612]\n",
      " [0.15932858]\n",
      " [0.33500835]\n",
      " [0.43895245]\n",
      " [0.40800086]\n",
      " [0.38047978]\n",
      " [0.54406804]\n",
      " [0.4453611 ]\n",
      " [0.4298856 ]\n",
      " [0.4091794 ]\n",
      " [0.41941556]\n",
      " [0.4441931 ]\n",
      " [0.23578978]\n",
      " [0.23258425]\n",
      " [0.31996366]\n",
      " [0.44016463]\n",
      " [0.3374025 ]\n",
      " [0.38419664]\n",
      " [0.42601463]\n",
      " [0.26618618]\n",
      " [0.29765   ]\n",
      " [0.4060168 ]\n",
      " [0.34976703]\n",
      " [0.2946631 ]\n",
      " [0.50598407]\n",
      " [0.4273528 ]\n",
      " [0.55495375]\n",
      " [0.3709795 ]\n",
      " [0.4087688 ]\n",
      " [0.4439208 ]\n",
      " [0.43942392]\n",
      " [0.40879524]\n",
      " [0.44691932]\n",
      " [0.26437253]\n",
      " [0.3268907 ]\n",
      " [0.34199527]\n",
      " [0.26915586]\n",
      " [0.45617616]\n",
      " [0.2345013 ]\n",
      " [0.3164131 ]\n",
      " [0.54122216]\n",
      " [0.4140624 ]\n",
      " [0.46898866]\n",
      " [0.37376237]\n",
      " [0.28457505]\n",
      " [0.3435535 ]\n",
      " [0.31336644]\n",
      " [0.27648744]\n",
      " [0.34283766]\n",
      " [0.3420488 ]\n",
      " [0.37314582]\n",
      " [0.40018094]\n",
      " [0.2212764 ]\n",
      " [0.34488422]\n",
      " [0.5116516 ]\n",
      " [0.28803918]\n",
      " [0.359249  ]\n",
      " [0.37747502]\n",
      " [0.29239455]\n",
      " [0.37837082]\n",
      " [0.30507997]\n",
      " [0.36341184]\n",
      " [0.5270611 ]\n",
      " [0.36181015]\n",
      " [0.33359873]\n",
      " [0.43780836]\n",
      " [0.3362747 ]\n",
      " [0.4290358 ]\n",
      " [0.5361577 ]\n",
      " [0.2327858 ]\n",
      " [0.4006048 ]\n",
      " [0.2317317 ]\n",
      " [0.38814571]\n",
      " [0.41956097]\n",
      " [0.3774168 ]\n",
      " [0.26884127]\n",
      " [0.41259637]\n",
      " [0.35017428]\n",
      " [0.37800193]\n",
      " [0.20386577]\n",
      " [0.3988628 ]\n",
      " [0.45564458]\n",
      " [0.35318124]\n",
      " [0.52859044]\n",
      " [0.20979384]\n",
      " [0.42701206]\n",
      " [0.54671276]\n",
      " [0.19589226]\n",
      " [0.33360556]\n",
      " [0.39352453]\n",
      " [0.2481046 ]\n",
      " [0.19217643]\n",
      " [0.41199702]\n",
      " [0.5068905 ]\n",
      " [0.45730513]\n",
      " [0.35241106]\n",
      " [0.40488413]\n",
      " [0.32561   ]\n",
      " [0.37577146]\n",
      " [0.45230043]\n",
      " [0.531327  ]\n",
      " [0.3898052 ]\n",
      " [0.41303197]\n",
      " [0.3350333 ]\n",
      " [0.55668914]\n",
      " [0.5279769 ]\n",
      " [0.4100237 ]\n",
      " [0.24452776]\n",
      " [0.3687667 ]\n",
      " [0.27498832]\n",
      " [0.4348051 ]\n",
      " [0.18004996]\n",
      " [0.20788601]\n",
      " [0.2967101 ]\n",
      " [0.38077763]\n",
      " [0.27748635]\n",
      " [0.30051643]\n",
      " [0.4491232 ]\n",
      " [0.3770655 ]\n",
      " [0.48421866]\n",
      " [0.5417948 ]\n",
      " [0.36675864]\n",
      " [0.15636472]\n",
      " [0.31180674]\n",
      " [0.46328986]\n",
      " [0.45363507]\n",
      " [0.36495903]\n",
      " [0.2540465 ]\n",
      " [0.47198275]\n",
      " [0.4855818 ]\n",
      " [0.22512467]\n",
      " [0.34813428]\n",
      " [0.43419486]\n",
      " [0.46856958]\n",
      " [0.45861888]\n",
      " [0.49748245]\n",
      " [0.44758445]\n",
      " [0.49052677]\n",
      " [0.37041712]\n",
      " [0.34365854]\n",
      " [0.3142444 ]\n",
      " [0.43665445]\n",
      " [0.47183314]\n",
      " [0.20457122]\n",
      " [0.41708738]\n",
      " [0.45925894]\n",
      " [0.25231108]\n",
      " [0.34223837]\n",
      " [0.48924875]\n",
      " [0.32573578]\n",
      " [0.550784  ]\n",
      " [0.20199476]\n",
      " [0.46856973]\n",
      " [0.33261034]\n",
      " [0.5017864 ]\n",
      " [0.26185003]\n",
      " [0.3532644 ]\n",
      " [0.41108167]\n",
      " [0.46511078]\n",
      " [0.17039911]\n",
      " [0.20265119]\n",
      " [0.37637898]\n",
      " [0.41315088]\n",
      " [0.2554615 ]\n",
      " [0.38973668]\n",
      " [0.31089652]\n",
      " [0.23166904]\n",
      " [0.4508288 ]\n",
      " [0.2757643 ]\n",
      " [0.56196564]\n",
      " [0.4169582 ]\n",
      " [0.3335145 ]\n",
      " [0.5171045 ]\n",
      " [0.38494015]\n",
      " [0.42019406]\n",
      " [0.23729768]\n",
      " [0.21066369]\n",
      " [0.43290454]\n",
      " [0.2486964 ]\n",
      " [0.28393343]\n",
      " [0.4765807 ]\n",
      " [0.5205268 ]\n",
      " [0.49826002]\n",
      " [0.5390348 ]\n",
      " [0.37496912]\n",
      " [0.48821968]\n",
      " [0.26837733]\n",
      " [0.26635125]\n",
      " [0.30433798]\n",
      " [0.5567109 ]\n",
      " [0.3545097 ]\n",
      " [0.17586349]\n",
      " [0.51803845]\n",
      " [0.41175804]\n",
      " [0.36815327]\n",
      " [0.40878737]\n",
      " [0.09220172]\n",
      " [0.4965688 ]\n",
      " [0.4196986 ]\n",
      " [0.40813017]\n",
      " [0.3874557 ]\n",
      " [0.58673203]\n",
      " [0.3549432 ]\n",
      " [0.41385818]\n",
      " [0.44277224]\n",
      " [0.44185945]\n",
      " [0.20621601]\n",
      " [0.37664178]\n",
      " [0.5066608 ]\n",
      " [0.3668504 ]\n",
      " [0.4126404 ]\n",
      " [0.5943951 ]\n",
      " [0.48178372]\n",
      " [0.49704874]\n",
      " [0.35025692]\n",
      " [0.42265484]\n",
      " [0.5534737 ]\n",
      " [0.40682685]\n",
      " [0.3839031 ]\n",
      " [0.22568586]\n",
      " [0.2733249 ]\n",
      " [0.31436488]\n",
      " [0.33635503]\n",
      " [0.33119535]\n",
      " [0.42763713]\n",
      " [0.33935338]\n",
      " [0.41468564]\n",
      " [0.4337477 ]\n",
      " [0.4009962 ]\n",
      " [0.3630897 ]\n",
      " [0.2778884 ]\n",
      " [0.33384168]\n",
      " [0.53962064]\n",
      " [0.44800055]\n",
      " [0.23440456]\n",
      " [0.28052497]\n",
      " [0.30233294]\n",
      " [0.1599261 ]\n",
      " [0.48102784]\n",
      " [0.19909847]\n",
      " [0.48653963]\n",
      " [0.47398135]\n",
      " [0.45340768]\n",
      " [0.3620984 ]\n",
      " [0.5108417 ]\n",
      " [0.27947628]\n",
      " [0.43795493]\n",
      " [0.5288801 ]\n",
      " [0.23382238]\n",
      " [0.2806957 ]\n",
      " [0.45355287]\n",
      " [0.472804  ]\n",
      " [0.37912145]\n",
      " [0.4400586 ]\n",
      " [0.43614894]\n",
      " [0.47320023]\n",
      " [0.20555465]\n",
      " [0.3934021 ]\n",
      " [0.51117647]\n",
      " [0.3919588 ]\n",
      " [0.46101272]\n",
      " [0.38155723]\n",
      " [0.49260268]\n",
      " [0.49421653]\n",
      " [0.52273625]\n",
      " [0.29568478]\n",
      " [0.28486237]\n",
      " [0.47321087]\n",
      " [0.39780277]\n",
      " [0.6118526 ]\n",
      " [0.383196  ]\n",
      " [0.39224914]\n",
      " [0.3048682 ]\n",
      " [0.38978127]\n",
      " [0.5454298 ]\n",
      " [0.56965125]\n",
      " [0.45875597]\n",
      " [0.3782095 ]\n",
      " [0.4033829 ]\n",
      " [0.40181342]\n",
      " [0.31026685]\n",
      " [0.44108066]\n",
      " [0.46239197]\n",
      " [0.49781826]\n",
      " [0.3486631 ]\n",
      " [0.41174933]\n",
      " [0.58603007]\n",
      " [0.3014205 ]\n",
      " [0.3119249 ]\n",
      " [0.37677348]\n",
      " [0.36499992]\n",
      " [0.39408183]\n",
      " [0.47266456]\n",
      " [0.509673  ]\n",
      " [0.19283685]\n",
      " [0.16199204]\n",
      " [0.3728123 ]\n",
      " [0.29125285]\n",
      " [0.19779997]\n",
      " [0.44779202]\n",
      " [0.50063473]\n",
      " [0.41088748]\n",
      " [0.5272994 ]\n",
      " [0.49738696]\n",
      " [0.40938175]\n",
      " [0.44386125]\n",
      " [0.41679707]\n",
      " [0.34124923]\n",
      " [0.45513937]\n",
      " [0.36136663]\n",
      " [0.15355048]\n",
      " [0.4860317 ]\n",
      " [0.47925165]\n",
      " [0.41798913]\n",
      " [0.5042297 ]\n",
      " [0.45187405]\n",
      " [0.49424785]\n",
      " [0.3099725 ]\n",
      " [0.3695622 ]\n",
      " [0.5153915 ]\n",
      " [0.40313226]\n",
      " [0.44877973]\n",
      " [0.47760868]\n",
      " [0.31192946]\n",
      " [0.42000386]\n",
      " [0.45396596]\n",
      " [0.3275902 ]\n",
      " [0.3434139 ]\n",
      " [0.18524121]\n",
      " [0.23148523]\n",
      " [0.45238888]\n",
      " [0.37253296]\n",
      " [0.39201266]\n",
      " [0.34159833]\n",
      " [0.5338245 ]\n",
      " [0.30008405]\n",
      " [0.46505633]\n",
      " [0.21496814]\n",
      " [0.5538801 ]\n",
      " [0.2720988 ]\n",
      " [0.39815757]\n",
      " [0.33641374]\n",
      " [0.47586742]\n",
      " [0.34420568]\n",
      " [0.19005667]\n",
      " [0.45470357]\n",
      " [0.55648535]\n",
      " [0.25020424]\n",
      " [0.50996643]\n",
      " [0.4474171 ]\n",
      " [0.49522796]\n",
      " [0.41980448]\n",
      " [0.27287564]\n",
      " [0.24616846]\n",
      " [0.3711129 ]\n",
      " [0.17981242]\n",
      " [0.5644599 ]\n",
      " [0.24198295]\n",
      " [0.4978393 ]\n",
      " [0.46255198]\n",
      " [0.25158423]\n",
      " [0.2029977 ]\n",
      " [0.37856477]\n",
      " [0.28669834]\n",
      " [0.464366  ]\n",
      " [0.39065152]\n",
      " [0.6405666 ]\n",
      " [0.34604874]\n",
      " [0.37192738]\n",
      " [0.39573273]\n",
      " [0.43781215]\n",
      " [0.14550726]\n",
      " [0.39982924]\n",
      " [0.4516548 ]\n",
      " [0.4504594 ]\n",
      " [0.38995993]\n",
      " [0.3200219 ]\n",
      " [0.34951687]\n",
      " [0.517432  ]\n",
      " [0.38672015]\n",
      " [0.4156021 ]\n",
      " [0.45120224]\n",
      " [0.4184772 ]\n",
      " [0.47444463]\n",
      " [0.37533674]\n",
      " [0.45041963]\n",
      " [0.47350758]\n",
      " [0.3766192 ]\n",
      " [0.5551298 ]\n",
      " [0.41456524]\n",
      " [0.34874576]\n",
      " [0.31547698]\n",
      " [0.4798071 ]\n",
      " [0.4526156 ]\n",
      " [0.27050745]\n",
      " [0.3190978 ]\n",
      " [0.20732261]\n",
      " [0.33323658]\n",
      " [0.44060203]\n",
      " [0.55384797]\n",
      " [0.41677663]\n",
      " [0.3781621 ]\n",
      " [0.41934133]\n",
      " [0.45679674]\n",
      " [0.2788856 ]\n",
      " [0.5361498 ]\n",
      " [0.33069745]\n",
      " [0.45532164]\n",
      " [0.26519316]\n",
      " [0.13578644]\n",
      " [0.22371139]\n",
      " [0.26181188]\n",
      " [0.3906728 ]\n",
      " [0.42405447]\n",
      " [0.32198933]\n",
      " [0.41222474]\n",
      " [0.44423336]\n",
      " [0.2953601 ]\n",
      " [0.25323159]\n",
      " [0.49886912]\n",
      " [0.4954852 ]\n",
      " [0.23895653]\n",
      " [0.41369367]\n",
      " [0.19575548]\n",
      " [0.29295382]\n",
      " [0.42735803]\n",
      " [0.3849083 ]\n",
      " [0.49094096]\n",
      " [0.6501208 ]\n",
      " [0.18730327]\n",
      " [0.38637823]\n",
      " [0.3494799 ]\n",
      " [0.31134877]\n",
      " [0.35921443]\n",
      " [0.40378502]\n",
      " [0.47945425]\n",
      " [0.36462277]\n",
      " [0.27978015]\n",
      " [0.39945474]\n",
      " [0.1931048 ]\n",
      " [0.34505206]\n",
      " [0.32434997]\n",
      " [0.53738785]\n",
      " [0.31652454]\n",
      " [0.28542694]\n",
      " [0.46018094]\n",
      " [0.3458408 ]\n",
      " [0.30024797]\n",
      " [0.37122893]\n",
      " [0.3673722 ]\n",
      " [0.23241642]\n",
      " [0.34530202]\n",
      " [0.4588988 ]\n",
      " [0.45445964]\n",
      " [0.36451846]\n",
      " [0.38969213]\n",
      " [0.24788006]\n",
      " [0.442316  ]\n",
      " [0.2982714 ]\n",
      " [0.3870848 ]\n",
      " [0.28739032]\n",
      " [0.3650165 ]\n",
      " [0.4627014 ]\n",
      " [0.19013108]\n",
      " [0.22418912]\n",
      " [0.46286562]\n",
      " [0.42973536]\n",
      " [0.4279239 ]\n",
      " [0.50251496]\n",
      " [0.40919393]\n",
      " [0.35005754]\n",
      " [0.36902776]\n",
      " [0.41444933]\n",
      " [0.37326953]\n",
      " [0.39166683]\n",
      " [0.28971946]\n",
      " [0.27595592]\n",
      " [0.50476646]\n",
      " [0.39500916]\n",
      " [0.3707054 ]\n",
      " [0.20013866]\n",
      " [0.46732655]\n",
      " [0.46965918]\n",
      " [0.4487848 ]\n",
      " [0.36724246]\n",
      " [0.51298386]\n",
      " [0.45468956]\n",
      " [0.41681084]\n",
      " [0.29008868]\n",
      " [0.47876844]\n",
      " [0.48145294]\n",
      " [0.27125108]\n",
      " [0.19466074]\n",
      " [0.41163832]\n",
      " [0.24094738]\n",
      " [0.41938797]\n",
      " [0.21341553]\n",
      " [0.3078365 ]\n",
      " [0.29680791]\n",
      " [0.3809957 ]\n",
      " [0.46559182]\n",
      " [0.16947061]\n",
      " [0.26612225]\n",
      " [0.32625723]\n",
      " [0.3092755 ]\n",
      " [0.33670062]\n",
      " [0.41564375]\n",
      " [0.18960361]\n",
      " [0.51264024]\n",
      " [0.19017188]\n",
      " [0.50838614]\n",
      " [0.42519417]\n",
      " [0.36638504]\n",
      " [0.42587802]\n",
      " [0.39819467]\n",
      " [0.48893973]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy:  0.4756258\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8,1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    feed = {X: x_data, Y: y_data}\n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict=feed)\n",
    "        if step%200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict=feed))\n",
    "            \n",
    "            \n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy], feed_dict=feed)\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hypothesis = tf.nn.softmax(tf.matmul(X,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $S(y_i)=\\frac{e^{y_{i}}}{\\sum_{j}{e^{y_{j}}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function: Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L = \\frac{1}{N}\\sum_i{D(S(WX_i+b), L_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y \\* tf.log(hypothesis), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.2487664\n",
      "200 0.8173158\n",
      "400 0.7068485\n",
      "600 0.64863193\n",
      "800 0.6088766\n",
      "1000 0.579187\n",
      "1200 0.5556563\n",
      "1400 0.53618675\n",
      "1600 0.5195519\n",
      "1800 0.50498545\n",
      "2000 0.49198386\n",
      "[[9.2184502e-01 7.7680856e-02 4.7417133e-04]\n",
      " [2.4683180e-01 5.6514239e-01 1.8802580e-01]\n",
      " [2.5869245e-02 4.7745518e-02 9.2638522e-01]] [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1, 2, 1, 1], [2, 1, 3, 2], [3, 1, 3, 4], [4, 1, 5, 5], [1, 7, 5, 5], [1, 2, 5, 6], [1, 6, 6, 6], [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 0], [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step%200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "    \n",
    "    # Test & one-hot encoding\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(X,W) + b)\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9],[1, 3, 4, 3],[1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.arg_max(all, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data and set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis,1), tf.argmax(Y,1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training epoch/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 2.881628494\n",
      "Epoch: 0002 cost = 1.132338709\n",
      "Epoch: 0003 cost = 0.885392590\n",
      "Epoch: 0004 cost = 0.764974299\n",
      "Epoch: 0005 cost = 0.690417598\n",
      "Epoch: 0006 cost = 0.637805840\n",
      "Epoch: 0007 cost = 0.599422685\n",
      "Epoch: 0008 cost = 0.569533025\n",
      "Epoch: 0009 cost = 0.545206735\n",
      "Epoch: 0010 cost = 0.525360373\n",
      "Epoch: 0011 cost = 0.508326395\n",
      "Epoch: 0012 cost = 0.493483272\n",
      "Epoch: 0013 cost = 0.481093842\n",
      "Epoch: 0014 cost = 0.469757486\n",
      "Epoch: 0015 cost = 0.459565252\n",
      "Accuracy:  0.8931\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Tensorflow Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c/total_batch\n",
    "        \n",
    "        print('Epoch:', '%04d' % (epoch+1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "        \n",
    "    # Repeat results on test dataset\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X:mnist.test.images, Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample image show and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngvp/5YoDYY0vJlMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x263a2c08080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples-1)\n",
    "    print(\"Label:\", sess.run(tf.argmax(mnist.test.labels[r:r+1], 1)))\n",
    "    #print(\"Precision:\", sess.run(tf.argmax(hypothesis,1), feed_dict={X:mnist.test.images[r:r+1]}))\n",
    "    \n",
    "    plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
