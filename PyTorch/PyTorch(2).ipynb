{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ y = \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6,6,100)\n",
    "y = 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8VPW9//HXJztZSEAg7KusKsoi7gpuxaXS624rWq/Kr1ptb7XWpb32Xttbe61XW1uXqrWtt7Zct1qqVEQL6i1VAdlkCYQ97JAQSEKWmfn8/kj0pghJSCY5s7yfj8c8mMl8Z/L+ksw7J9+cOcfcHRERSSwpQQcQEZHoU7mLiCQglbuISAJSuYuIJCCVu4hIAlK5i4gkIJW7iEgCUrmLiCQglbuISAJKC+oTd+vWzQcOHNiqx1ZWVpKTkxPdQAHRXGJPoswDNJdY1Za5LFy4cLe7d29uXGDlPnDgQBYsWNCqx86dO5eJEydGN1BANJfYkyjzAM0lVrVlLma2sSXjtCwjIpKAVO4iIglI5S4ikoBU7iIiCajZcjez58xsp5l9cpj7zcweM7NiM1tqZmOjH1NERI5ES7bcfwNMbuL+C4ChDZdpwJNtjyUiIm3RbLm7+3tAaRNDpgDPe70PgAIz6xWtgCIicuSisZ97H2Bzo9slDR/bFoXnFhGJCe5OTSjCgdow1aEwNXWRz/6tCUWoDUWoCYWpDUWoDdffDkWcuobrdWEnFI5QF45QcCDMxHbOG41yt0N87JAnZjWzadQv3VBYWMjcuXNb9QkrKipa/dhYo7nEnkSZB2guUF/K1WGoqHUq65zKOqgMOVV1TlXIqaqDAyGnOlT/b024/npNuP5xNWGnJgy14ejN5eoh3u5fl2iUewnQr9HtvsDWQw1096eBpwHGjx/vrX2Hlt6pFpsSZS6JMg9I7LnUhSPs2FfN9vJqtpVXs2NfNbsqati1r4bdlbXs3l/DnsoayirrqA1HDvu8KQa5mWnkZaWTm5lGTk4qXTPTyMlIIzsjlU4ZqfX/pqeS9em/6alkpaeQmZZKZloKWempZKSlkJGaUv9vWgrpKf93PS3VyEhNIS3FSE0x3n333Xb/ukSj3GcAt5nZdOAkoNzdtSQjIm1WWlnLul0VbNhTxXtranlt+yJKyg5QUnaAHfur8YPWCDJSU+iel0m3vEx65WdxTO/OdM3NoGt2Bl2yMyjITqcgO4P8Tul07pRG56x0sjNSMTvUAkR8a7bczewPwESgm5mVAN8H0gHc/SlgJnAhUAxUATe0V1gRSUzlVXWs2LaPou37KNqxn9U7Kli7q4K9VXWfjTGgd0EZ/bp24vSh3ehT0IneBVn0zO9Ez85ZFHbOJL9TekIWdWs0W+7ufk0z9zvw9aglEpGEdqA2zLIt5Xy8qYylJXtZtqWczaUHPru/IDudYYV5XHhcL4Z0z2VwtxwGHJXNumXzOffsSQEmjy+BHRVSRJLDvuo65q8v5cP1pXy4bg+fbN1HOFK/ntK/azaj+xRwzYT+HNM7nxE98+iRl3nIre9NKdoiPxIqdxGJqkjEWbalnLlFu3h/zS4Wbd5LOOJkpKZwQr8CvnbWYMb278IJ/Qo4Kjcz6LgJS+UuIm1WF44wb+0eZi3fzjsrd7BjXw1mcFyffG45awinHd2NMf0LyEpPDTpq0lC5i0iruDvzN5Txp8Vb+Msn2ymtrCU7I5Uzh3bnvFGFTBrRg645GUHHTFoqdxE5Ijv2VfPywhJeWrCZDXuqyEpP4dyRhXzx+N6cNay7ts5jhMpdRJr16Vb6b+dt4M3l2wlHnJMGdeX2s4cy+die5GSqSmKNviIicljhiDNz2Taeencty7fuI79TOjedPohrJvRnYLfEOFl1olK5i8jn1IUjvPpxCU/OXcuGPVUM6Z7Dg5cex5dO6EOnDC27xAOVu4h8JhJx/rx0K4/OXs2GPVUc26czT107lvNH9SRF+5nHFZW7iAAwb+1ufvj6SlZs28eInnk8e914zhnZQ2/nj1Mqd5EkV1JWxY9mrmTmsu30KejET686gUuO760t9TinchdJUqFwhGfeX89P316NGdxx3jCmnTlYuzImCJW7SBJavrWcu19Zyidb9nH+qEK+f8kx9CnoFHQsiSKVu0gSCYUjPD5nLY/9dQ1dsjN48itjueA4nfI4EancRZLE5tIqvvU/i1mwsYwpJ/Tm3y85hoJsHR4gUancRZLAzGXbuPvlpQD89KoT+NKYPgEnkvamchdJYOGI86OZK3n6vXWc0K+An18zhn5ds4OOJR1A5S6SoHZX1PCTBdWsKl3H1JMH8K8XjyIjLSXoWNJBVO4iCWjNjv3c8Jv57CiP8F9XHM9l4/oGHUk6mH6MiySYecW7ufTJeVTXRbj3pCwVe5JSuYskkFc/LuG65z6iV34Wr339VAbn6w1JyUrlLpIgfjtvA3e8uIQJg7ry8i2n0reL/nCazLTmLhLn3J3H5xTz8FurOX9UIY9dM0aHEBCVu0g8c3cemlXEk3PXcumYPjx0+WjSUvULuajcReLaI7NX8+TctXz5pP78cMqxOpKjfEY/4kXi1GPvrOHnfy3m6hP7qdjlc1TuInHol++u5ZHZq7lsbF9+9E/Hqdjlc1TuInHmpQWbefAvq7h4dC8euny0il0OSeUuEkfmrNrJPa8u4/Sju/HIlSeQqmKXw1C5i8SJRZvKuPWFjxnZK4+npo7TcWKkSfruEIkDm0uruOm3C+iel8mvvzqB3Ezt6CZNa1G5m9lkMysys2Izu+cQ9/c3szlmtsjMlprZhdGPKpKcKmpC3Pz8AmrDEX59w4l0z8sMOpLEgWbL3cxSgceBC4BRwDVmNuqgYd8DXnT3McDVwBPRDiqSjCIR51+mL2bNzgqe+MpYhnTPDTqSxImWbLlPAIrdfZ271wLTgSkHjXGgc8P1fGBr9CKKJK+fvFXE2yt3cP/FozhjaPeg40gcMXdveoDZ5cBkd7+p4fZU4CR3v63RmF7AW0AXIAc4190XHuK5pgHTAAoLC8dNnz69VaErKirIzU2MLRjNJfbEyjzmbw/x+OIaJvZL4/pRGZgd+Z4xsTKXaNBc6k2aNGmhu49vdqC7N3kBrgCebXR7KvDzg8bcAdzZcP0UYAWQ0tTzjhs3zltrzpw5rX5srNFcYk8szKN4534/5v43fcov/tdr6sKtfp5YmEu0aC71gAXeTG+7e4uWZUqAfo1u9+Xzyy43Ai82/LD4O5AFdGvBc4vIQapqQ9zyu4VkpKXwxFfGapdHaZWWfNfMB4aa2SAzy6D+D6YzDhqzCTgHwMxGUl/uu6IZVCQZuDv3vbqMNTsreOzqMfQu6BR0JIlTzZa7u4eA24BZwErq94pZbmYPmNklDcPuBG42syXAH4CvNvz6ICJH4KWFJby2eCt3nDuM04fql19pvRa9E8LdZwIzD/rY/Y2urwBOi240keSyblcF/zZjOacMPopbJx0ddByJc1rME4kBNaEw35i+iIy0FB69SseMkbbTe5hFYsDDs4r4ZMs+np46jp75WUHHkQSgLXeRgM1bu5tn3l/PV07qz/nH9Aw6jiQIlbtIgPZX13HXS0sZ1C2H71108FE9RFpPyzIiAfqPN1ayrfwAL33tFDplpAYdRxKIttxFAjKnaCfT52/m5jMHM25A16DjSIJRuYsEoLyqjnteWcqwwly+de6woONIAtKyjEgAHvzLSnbtr+GZ68aTla7lGIk+bbmLdLC/r91TvxxzxmBG9y0IOo4kKJW7SAeqrgtz3x+X0b9rNv+i5RhpR1qWEelAj72zhvW7K3nhppO0d4y0K225i3SQldv28cv31nHl+L6cdrQOCibtS+Uu0gEiEed7r31Cfqd07rtwZNBxJAmo3EU6wMsfl7BwYxn3XjCCguyMoONIElC5i7SzsspaHpy5kvEDunDZ2L5Bx5EkoXIXaWcPzSpiX3WIH3zpWFJ0KF/pICp3kXa0ePNeps/fxA2nDmRkr85Bx5EkonIXaSeRiPP9GcvplpvJN88dGnQcSTIqd5F28sdFW1iyeS93Tx5BXlZ60HEkyajcRdpBRU2IH7+5iuP7FXDpmD5Bx5EkpHeoirSDX/y1mF37a3h66jj9EVUCoS13kSjbuKeS5/53PZeN7cuY/l2CjiNJSuUuEmUPzlxFWqpx9+ThQUeRJKZyF4mij9aX8uby7dxy1hB6dM4KOo4kMZW7SJREIs5/vLGCnp2zuOmMwUHHkSSncheJkhlLtrKkpJy7vjBch/OVwKncRaKgui7MQ2+u4tg+nfkn7fooMUDlLhIFz/1tPVvLq/nuhaO066PEBJW7SBuVVdby5Ny1nDOiB6cMOSroOCKAyl2kzR6fU0xlTYjvTB4RdBSRz6jcRdpgc2kVz/99I5eN7cvwnnlBxxH5TIvK3cwmm1mRmRWb2T2HGXOlma0ws+Vm9vvoxhSJTY/OXo0ZfOu8YUFHEfkHzR5bxsxSgceB84ASYL6ZzXD3FY3GDAXuBU5z9zIz69FegUVixYqt+/jj4i1MO3MwvQs6BR1H5B+0ZMt9AlDs7uvcvRaYDkw5aMzNwOPuXgbg7jujG1Mk9jw0axWds9K59ayjg44i8jnm7k0PMLscmOzuNzXcngqc5O63NRrzGrAaOA1IBf7N3d88xHNNA6YBFBYWjps+fXqrQldUVJCbm9uqx8YazSX2tGQeRaVhHvyomiuHpXPh4Ng94XWifE1Ac/nUpEmTFrr7+GYHunuTF+AK4NlGt6cCPz9ozOvAH4F0YBD1yzcFTT3vuHHjvLXmzJnT6sfGGs0l9jQ3j0gk4pc98Tc/8Yezvaom1DGhWilRvibumsungAXeTG+7e4uWZUqAfo1u9wW2HmLMn9y9zt3XA0WAzismCWlO0U4WbCzjG+cM1WEGJGa1pNznA0PNbJCZZQBXAzMOGvMaMAnAzLoBw4B10QwqEgsiEeehN4sYcFQ2V53Yr/kHiASk2XJ39xBwGzALWAm86O7LzewBM7ukYdgsYI+ZrQDmAHe5+572Ci0SlD8v3cqq7fu547xhpKfqbSISu1p0mj13nwnMPOhj9ze67sAdDReRhFQXjvDo7NWM6JnHF0f3DjqOSJO06SHSQq8sLGHDniq+ff5wHRxMYp7KXaQFakJhHntnDcf3K+CckXqPnsQ+lbtIC/zhw01sLa/mrvOHY6atdol9KneRZlTVhvjFnLWcPLgrpx2tQ/pKfFC5izTj+b9vZHdFDd/WVrvEEZW7SBP2V9fx1LtrOWtYd8YP7Bp0HJEWU7mLNOHXf9vA3qo67jxfh/SV+KJyFzmM8qo6nnl/HeeNKmR034Kg44gcEZW7yGE88/469leHuEMn4pA4pHIXOYQ9FTX8+m/ruWh0L0b26hx0HJEjpnIXOYRfvreOA3VhvnWuDm4q8UnlLnKQvTURnv/7Bqac0Ieje+ik1xKfVO4iB3ljXR11Yeeb52irXeKXyl2kkW3lB5izOcRlY/swsFtO0HFEWk3lLtLI43OKcYfbz9ZWu8Q3lbtIg5KyKv5n/mbO7JtGv67ZQccRaROVu0iDn79TjGFcPDg96CgibaZyFwE27K7k5Y9L+PJJ/Tmqk14WEv/0XSwCPPbOGtJSjFsnDgk6ikhUqNwl6RXv3M9ri7dw/akD6dE5K+g4IlGhcpek9+jba8hKT+X/nTk46CgiUaNyl6S2cts+3li6jRtOG8hRuZlBxxGJGpW7JLVHZ68mLzONm8/QVrskFpW7JK2lJXt5a8UObjxjEAXZGUHHEYkqlbskrYffWk2X7HRuPH1Q0FFEok7lLknpo/WlvLd6F187awh5WXrTkiQelbskHXfn4VlFdM/L5LpTBgYdR6RdqNwl6by/ZjcfbSjl9rOPplNGatBxRNqFyl2SirvzX28V0aegE1ed2C/oOCLtRuUuSWXW8h0sKSnnm+cMJTNNW+2SuFpU7mY22cyKzKzYzO5pYtzlZuZmNj56EUWiIxxxHn6riCHdc7h0bJ+g44i0q2bL3cxSgceBC4BRwDVmNuoQ4/KAbwAfRjukSDS8+nEJxTsr+Pb5w0lL1S+tktha8h0+ASh293XuXgtMB6YcYtwPgIeA6ijmE4mKmlCYn769htF985l8bM+g44i0u5aUex9gc6PbJQ0f+4yZjQH6ufvrUcwmEjW//3ATW/Ye4K4vDMfMgo4j0u7SWjDmUK8E/+xOsxTgUeCrzT6R2TRgGkBhYSFz585tUciDVVRUtPqxsUZzaX8HQs6j71UxsmsKoZJPmLul6XKP1Xm0huYSmzpkLu7e5AU4BZjV6Pa9wL2NbucDu4ENDZdqYCswvqnnHTdunLfWnDlzWv3YWKO5tL9H3iryAXe/7os2lbVofKzOozU0l9jUlrkAC7yZ3nb3Fi3LzAeGmtkgM8sArgZmNPrhUO7u3dx9oLsPBD4ALnH3BdH44SPSFrv21/DM++u46LhenNCvIOg4Ih2m2XJ39xBwGzALWAm86O7LzewBM7ukvQOKtMVj76yhNhTh218YHnQUkQ7VkjV33H0mMPOgj91/mLET2x5LpO3W767kDx9t4poJ/RnULSfoOCIdSjv7SsJ6eFYRGWkpfOOcoUFHEelwKndJSIs2lfHGsm3cdMZguufp9HmSfFTuknDcnR++sZLueZk66bUkLZW7JJyZy7azcGMZd543jJzMFv1ZSSThqNwlodSEwvz4zZWM6JnHFeN1SF9JXip3SSjPz9vI5tID3HfhSFJTdJgBSV4qd0kYZZW1/PyvazhrWHfOHNY96DgigVK5S8L4r9lFVNaG+e5FI4OOIhI4lbskhBVb9/H7Dzcx9eQBDCvMCzqOSOBU7hL33J1///Ny8jul861zhwUdRyQmqNwl7s1ctp0P15dyx/nDyc9ODzqOSExQuUtcq64L86OZ9bs+fnlC/6DjiMQMvcND4toTc4rZsvcAf7j5ZO36KNKIttwlbq3bVcFT767jSyf05pQhRwUdRySmqNwlLrk735+xnMy0FO7Tro8in6Nyl7g0c9l23l+zmzvPH0aPvKyg44jEHJW7xJ2KmhAPvL6cY3p35tqTBwQdRyQm6Q+qEncenlXEzv01PHntONJStX0icih6ZUhc+XhTGb/9+wauO3kAY/t3CTqOSMxSuUvcqA1FuOeVpfTsnMVdk0cEHUckpmlZRuLGk3PXsnpHBb+6fjy5OgmHSJO05S5xYc2O/Tw+p5gvHt+bc0YWBh1HJOap3CXmhcIRvv3SErIzU7n/4lFBxxGJC/rdVmLeU++uZUlJOb/48hi652UGHUckLmjLXWLaiq37+Nk7a7h4dC8uHt076DgicUPlLjGrNhThjhcXk98pgx9MOTboOCJxRcsyErMefXs1q7bv55nrxtMlJyPoOCJxRVvuEpPmFe/mqXfXcvWJ/ThvlPaOETlSKneJOaWVtXzrxcUM7pbD/V/U3jEiraFlGYkp7s53Xl5KWWUdz331RLIz9C0q0hracpeY8t8fbOTtlTu4+4IRHNM7P+g4InGrReVuZpPNrMjMis3snkPcf4eZrTCzpWb2jpnpOKxyxBZv3ssPXl/B2SN68M+nDQw6jkhca7bczSwVeBy4ABgFXGNmBy+ELgLGu/to4GXgoWgHlcRWWlnLrb9bSGHnLB658njMdD5UkbZoyZb7BKDY3de5ey0wHZjSeIC7z3H3qoabHwB9oxtTElk44nxz+iJ2V9by1LXjKMjWbo8ibdWScu8DbG50u6ThY4dzI/CXtoSS5PLo7NW8v2Y3P5hyDMf20Tq7SDSYuzc9wOwK4AvuflPD7anABHe//RBjrwVuA85y95pD3D8NmAZQWFg4bvr06a0KXVFRQW5ubqseG2uSfS4fbA3x1NIazuybxj8fGxvHjUn2r0ms0lzqTZo0aaG7j292oLs3eQFOAWY1un0vcO8hxp0LrAR6NPec7s64ceO8tebMmdPqx8aaZJ7Lok1lPvS7M/2Kp+Z5TV24fUK1QjJ/TWKZ5lIPWOAt6NiWLMvMB4aa2SAzywCuBmY0HmBmY4BfApe4+86W/gSS5LWt/AA3P7+AHnmZPHXtODLStFeuSDQ1+4py9xD1Sy2zqN8yf9Hdl5vZA2Z2ScOwnwC5wEtmttjMZhzm6UTYX13Hjb9ZQFVNiF9dfyJdddwYkahr0dv/3H0mMPOgj93f6Pq5Uc4lCaomFOb//fdCVu/Yz7PXj2d4z7ygI4kkJL23WzpMJOLc+eIS5q3dwyNXHs/E4T2CjiSSsLTQKR3C3Xng9RW8vnQb914wgkvH6q0QIu1J5S7tzt358Zur+M28Ddx4+iCmnTk46EgiCU/lLu3u0dmr+eW767j25P5876KROrSASAdQuUu7euydNTz212KuGt+PBy45VsUu0kH0B1VpF+7Of75ZxFPvruXSsX148NLjSElRsYt0FJW7RF0k4vzrnz7hhQ838ZWT+vODKceq2EU6mMpdoqomFOY7Ly/lT4u38rWzhnD35OFaihEJgMpdoqai1rnuVx/x4fpSvjN5OLdOPDroSCJJS+UuUbFxTyU//OAApTXV/OzqE5hyQlNHhRaR9qZylzb7W/Fubvv9x9TWOS/cfDInDuwadCSRpKdyl1Zzd558dy0PzypiSPdcbhyepmIXiRHaz11apbyqjq/9biEPvVnEBcf14rWvn0bPHH07icQKbbnLEftg3R7u+J/F7Nxfw/cuGsmNpw/SHjEiMUblLi1WG4rws3dW88TctQzoms0rt5zK8f0Kgo4lIoegcpcWWbSpjLtfWcrqHRVcOb4v3//iMeRk6ttHJFbp1SlNqqgJ8ejs1Tz3t/X07JzFc18dz9kjCoOOJSLNULnLIbk7ry3ewoMzV7Fzfw3XntyfuyePIC8rPehoItICKnf5nIUbS/mPN1by8aa9HN83n19OHceY/l2CjiUiR0DlLp9ZuW0fD88q4p1VO+mWm8lDl4/m8rF9ddAvkTikchc+2VLOE3OL+csn28nNTOOuLwznhtMGkp2hbw+ReKVXb5Jyd+at3cPT763j3dW7yMtM49aJQ7j5jMEUZGcEHU9E2kjlnmQqa0L8cdEWfjtvA2t2VnBUTgZ3fWE4U08ZQGf9sVQkYajck4C7s3BjGS8u2MzrS7dRVRvmuD75PHzF8Vw8uhdZ6alBRxSRKFO5J7BV2/fx5yVb+fOSbWwqrSInI5Uvju7NlSf2Y2z/Ah0yQCSBqdwTSCTiLNpcxlsrdjB7xQ7W7aokxeC0o7tx+9lHc+FxvfSuUpEkoVd6nNteXs37a3bx3prd/O+aXZRV1ZGWYpw8+ChuOHUgFxzXi265mUHHFJEOpnKPI5GIs35PJR9vLGP+hlI+XF/Kxj1VAHTLzWTS8B6cNbw7E4f3IL+T/jgqksxU7jEqEnG27D3A8q3lLNtSzrIt+1iyeS/lB+oAyO+UzoRBXZl68gBOHdKNkb3ytIYuIp9RuQcsHHE27qlk3a5K1u6qoHhnBUU79rN6+34qa8MApKYYQ3vkcsGxPRnbvwtj+hcwpHuu3jkqIoelcm9ntaEIO/ZVs628mm3lBygp+/RSxabSKkpKqwi/Nfez8V1zMhjaI5fLx/VleM/OjOrdmRE987S7oogckRaVu5lNBn4GpALPuvuPD7o/E3geGAfsAa5y9w3RjRob6sIRyg/UsbeqjvIDtZRW1lFWVUtpZS17KmrYU1HLrooadu2vYef+Gkoraz/3HF1zMuhT0Inj+uRzXH4dZ44ZyZAeOQzulkuXHL07VETartlyN7NU4HHgPKAEmG9mM9x9RaNhNwJl7n60mV0N/CdwVXsEbo67Uxd2asMRakMRakJhakMRqusiVNeF6y+h/7t+oDZMVW2YA3VhKmtCVNXW/1vR6LK/OsT+6jr2V9fffzhZ6Sl0y83kqNxM+nXNZtyALnTPy6R3fid65mfRKz+LPl06/cMxW+bOncvEE/t1xH+NiCSRlmy5TwCK3X0dgJlNB6YAjct9CvBvDddfBn5hZubuHsWsALw4fzOPvl9F+kdzCIUj1IadUCRCXSjyWam3VmqKkZORSnZGGnlZaeRk1v/bs3MWnbPSyctKo3OndAqy08nvVH/pmpNBl+wMuuRkkJORqj9qikhMaEm59wE2N7pdApx0uDHuHjKzcuAoYHfjQWY2DZgGUFhYyNy5c4848JadIXp1ipCZXkNqipFqkJYCaSlGqqWQnpJKagqkpxjpKZCeUn9/Ruqnt42MVMhIrb+emQqZqUZmGqQZjcrZgbqGy0FqGi57oZT6S2tVVFS06v8hFiXKXBJlHqC5xKqOmEtLyv1Qm6IHb5G3ZAzu/jTwNMD48eN94sSJLfj0/2giMGbuXFrz2Fg0V3OJOYkyD9BcYlVHzCWlBWNKgMaLwn2BrYcbY2ZpQD5t26AVEZE2aEm5zweGmtkgM8sArgZmHDRmBnB9w/XLgb+2x3q7iIi0TLPLMg1r6LcBs6jfFfI5d19uZg8AC9x9BvAr4L/NrJj6Lfar2zO0iIg0rUX7ubv7TGDmQR+7v9H1auCK6EYTEZHWasmyjIiIxBmVu4hIAlK5i4gkIJW7iEgCsqD2WDSzXcDGVj68Gwe9+zWOaS6xJ1HmAZpLrGrLXAa4e/fmBgVW7m1hZgvcfXzQOaJBc4k9iTIP0FxiVUfMRcsyIiIJSOUuIpKA4rXcnw46QBRpLrEnUeYBmkusave5xOWau4iINC1et9xFRKQJcV3uZna7mRWZ2XIzeyjoPG1lZt82MzezbkFnaQ0z+4mZrTKzpWb2RzMrCDrTkTKzyQ3fU8Vmdk/QeVrLzPqZ2RwzW9nw+vhm0JnawsxSzWyRmb0edJa2MLMCM3u54XWy0sxOaa/PFbflbmaTqD+932h3PwZ4OOBIbWJm/ag/T+2moLO0wWzgWHcfDawG7g04zxFpdL7gC4BRwDVmNirYVK0WAu5095HAycDX43guAN8EVgYdIgp+Brzp7iOA42nHOcVtuQO3AD929xoAd98ZcJ62ehT4Doc4g1W8cPe33D3UcPMD6k/sEk8+O1+wu9cCn54vOO64+zZ3/7jh+n7qS6RPsKlax8z6AhcBzwadpS3MrDNwJvWHSMfda919b3t9vngdmNecAAACGElEQVQu92HAGWb2oZm9a2YnBh2otczsEmCLuy8JOksU/TPwl6BDHKFDnS84LguxMTMbCIwBPgw2Sav9lPoNn0jQQdpoMLAL+HXDEtOzZpbTXp+sRcdzD4qZvQ30PMRd36U+exfqf+U8EXjRzAbH6hmgmpnLfcD5HZuodZqah7v/qWHMd6lfFnihI7NFQYvOBRxPzCwXeAX4F3ffF3SeI2VmFwM73X2hmU0MOk8bpQFjgdvd/UMz+xlwD/Cv7fXJYpa7n3u4+8zsFuDVhjL/yMwi1B+vYVdH5TsSh5uLmR0HDAKWmBnUL2V8bGYT3H17B0Zskaa+JgBmdj1wMXBOrP6gbUJLzhccN8wsnfpif8HdXw06TyudBlxiZhcCWUBnM/udu18bcK7WKAFK3P3T36Bepr7c20U8L8u8BpwNYGbDgAzi8KBC7r7M3Xu4+0B3H0j9N8DYWCz25pjZZOBu4BJ3rwo6Tyu05HzBccHqtxR+Bax090eCztNa7n6vu/dteG1cTf35meOx2Gl4TW82s+ENHzoHWNFeny+mt9yb8RzwnJl9AtQC18fhlmKi+QWQCcxu+C3kA3f/WrCRWu5w5wsOOFZrnQZMBZaZ2eKGj93XcMpMCc7twAsNGw/rgBva6xPpHaoiIgkonpdlRETkMFTuIiIJSOUuIpKAVO4iIglI5S4ikoBU7iIiCUjlLiKSgFTuIiIJ6P8DKrt2yDdLRqQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a5c9b6128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 下载MNIST训练集\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, \\\n",
    "                              transform=transforms.ToTensor(), \\\n",
    "                              download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, \\\n",
    "                             transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \\\n",
    "                         shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \\\n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, in_dim, n_class):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.logistic = nn.Linear(in_dim, n_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.logistic(x)\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression (\n",
       "  (logistic): Linear (784 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(28*28, 10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义loss和optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression (\n",
       "  (logistic): Linear (784 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "epoch 1\n",
      "[1/100] Loss: 0.434512, Acc: 0.883646\n",
      "[1/100] Loss: 0.434366, Acc: 0.887135\n",
      "[1/100] Loss: 0.432979, Acc: 0.887500\n",
      "[1/100] Loss: 0.432539, Acc: 0.886719\n",
      "[1/100] Loss: 0.430482, Acc: 0.887083\n",
      "[1/100] Loss: 0.430552, Acc: 0.886840\n",
      "Finish 1 epoch, Loss: 0.432324, Acc: 0.886517\n",
      "Test Loss: 0.408021, Acc: 0.895300\n",
      "Time:7.0 s\n",
      "\n",
      "**********\n",
      "epoch 2\n",
      "[2/100] Loss: 0.432882, Acc: 0.885312\n",
      "[2/100] Loss: 0.431764, Acc: 0.883906\n",
      "[2/100] Loss: 0.435905, Acc: 0.883125\n",
      "[2/100] Loss: 0.432827, Acc: 0.884661\n",
      "[2/100] Loss: 0.428580, Acc: 0.886542\n",
      "[2/100] Loss: 0.426381, Acc: 0.887934\n",
      "Finish 2 epoch, Loss: 0.426718, Acc: 0.887533\n",
      "Test Loss: 0.402901, Acc: 0.896200\n",
      "Time:7.4 s\n",
      "\n",
      "**********\n",
      "epoch 3\n",
      "[3/100] Loss: 0.430750, Acc: 0.887188\n",
      "[3/100] Loss: 0.426994, Acc: 0.888333\n",
      "[3/100] Loss: 0.423445, Acc: 0.888507\n",
      "[3/100] Loss: 0.424479, Acc: 0.887578\n",
      "[3/100] Loss: 0.423400, Acc: 0.888208\n",
      "[3/100] Loss: 0.421896, Acc: 0.888576\n",
      "Finish 3 epoch, Loss: 0.421591, Acc: 0.888533\n",
      "Test Loss: 0.398093, Acc: 0.896800\n",
      "Time:7.5 s\n",
      "\n",
      "**********\n",
      "epoch 4\n",
      "[4/100] Loss: 0.418081, Acc: 0.889687\n",
      "[4/100] Loss: 0.413463, Acc: 0.891615\n",
      "[4/100] Loss: 0.412505, Acc: 0.891111\n",
      "[4/100] Loss: 0.414702, Acc: 0.890495\n",
      "[4/100] Loss: 0.415436, Acc: 0.890542\n",
      "[4/100] Loss: 0.416711, Acc: 0.889583\n",
      "Finish 4 epoch, Loss: 0.416851, Acc: 0.889400\n",
      "Test Loss: 0.393741, Acc: 0.897100\n",
      "Time:8.1 s\n",
      "\n",
      "**********\n",
      "epoch 5\n",
      "[5/100] Loss: 0.423471, Acc: 0.886771\n",
      "[5/100] Loss: 0.420801, Acc: 0.888177\n",
      "[5/100] Loss: 0.416637, Acc: 0.889097\n",
      "[5/100] Loss: 0.415738, Acc: 0.889818\n",
      "[5/100] Loss: 0.414741, Acc: 0.889750\n",
      "[5/100] Loss: 0.413170, Acc: 0.890226\n",
      "Finish 5 epoch, Loss: 0.412470, Acc: 0.890367\n",
      "Test Loss: 0.389842, Acc: 0.898100\n",
      "Time:6.8 s\n",
      "\n",
      "**********\n",
      "epoch 6\n",
      "[6/100] Loss: 0.408140, Acc: 0.893750\n",
      "[6/100] Loss: 0.409733, Acc: 0.892188\n",
      "[6/100] Loss: 0.407610, Acc: 0.892639\n",
      "[6/100] Loss: 0.408891, Acc: 0.891589\n",
      "[6/100] Loss: 0.407472, Acc: 0.892208\n",
      "[6/100] Loss: 0.408190, Acc: 0.891597\n",
      "Finish 6 epoch, Loss: 0.408405, Acc: 0.891400\n",
      "Test Loss: 0.385941, Acc: 0.897900\n",
      "Time:7.6 s\n",
      "\n",
      "**********\n",
      "epoch 7\n",
      "[7/100] Loss: 0.405544, Acc: 0.892708\n",
      "[7/100] Loss: 0.411206, Acc: 0.889635\n",
      "[7/100] Loss: 0.410935, Acc: 0.889792\n",
      "[7/100] Loss: 0.407002, Acc: 0.890573\n",
      "[7/100] Loss: 0.405127, Acc: 0.891813\n",
      "[7/100] Loss: 0.403516, Acc: 0.892170\n",
      "Finish 7 epoch, Loss: 0.404599, Acc: 0.891933\n",
      "Test Loss: 0.382566, Acc: 0.898700\n",
      "Time:7.1 s\n",
      "\n",
      "**********\n",
      "epoch 8\n",
      "[8/100] Loss: 0.404736, Acc: 0.892604\n",
      "[8/100] Loss: 0.399783, Acc: 0.892708\n",
      "[8/100] Loss: 0.400960, Acc: 0.892986\n",
      "[8/100] Loss: 0.402246, Acc: 0.892083\n",
      "[8/100] Loss: 0.402415, Acc: 0.891708\n",
      "[8/100] Loss: 0.400405, Acc: 0.892726\n",
      "Finish 8 epoch, Loss: 0.401060, Acc: 0.892617\n",
      "Test Loss: 0.379266, Acc: 0.899300\n",
      "Time:7.2 s\n",
      "\n",
      "**********\n",
      "epoch 9\n",
      "[9/100] Loss: 0.395625, Acc: 0.892396\n",
      "[9/100] Loss: 0.397257, Acc: 0.892760\n",
      "[9/100] Loss: 0.399923, Acc: 0.892674\n",
      "[9/100] Loss: 0.397088, Acc: 0.893620\n",
      "[9/100] Loss: 0.398510, Acc: 0.893354\n",
      "[9/100] Loss: 0.397950, Acc: 0.893212\n",
      "Finish 9 epoch, Loss: 0.397723, Acc: 0.893433\n",
      "Test Loss: 0.376261, Acc: 0.900500\n",
      "Time:6.3 s\n",
      "\n",
      "**********\n",
      "epoch 10\n",
      "[10/100] Loss: 0.400371, Acc: 0.891354\n",
      "[10/100] Loss: 0.402685, Acc: 0.890729\n",
      "[10/100] Loss: 0.396871, Acc: 0.892639\n",
      "[10/100] Loss: 0.396517, Acc: 0.892969\n",
      "[10/100] Loss: 0.395204, Acc: 0.893917\n",
      "[10/100] Loss: 0.394862, Acc: 0.893576\n",
      "Finish 10 epoch, Loss: 0.394593, Acc: 0.894033\n",
      "Test Loss: 0.373367, Acc: 0.901200\n",
      "Time:7.3 s\n",
      "\n",
      "**********\n",
      "epoch 11\n",
      "[11/100] Loss: 0.391709, Acc: 0.894792\n",
      "[11/100] Loss: 0.397994, Acc: 0.891354\n",
      "[11/100] Loss: 0.389251, Acc: 0.894826\n",
      "[11/100] Loss: 0.393013, Acc: 0.894036\n",
      "[11/100] Loss: 0.390720, Acc: 0.894563\n",
      "[11/100] Loss: 0.392038, Acc: 0.894167\n",
      "Finish 11 epoch, Loss: 0.391634, Acc: 0.894383\n",
      "Test Loss: 0.370613, Acc: 0.901700\n",
      "Time:7.0 s\n",
      "\n",
      "**********\n",
      "epoch 12\n",
      "[12/100] Loss: 0.381746, Acc: 0.897604\n",
      "[12/100] Loss: 0.387304, Acc: 0.897396\n",
      "[12/100] Loss: 0.394854, Acc: 0.893785\n",
      "[12/100] Loss: 0.392580, Acc: 0.893516\n",
      "[12/100] Loss: 0.390260, Acc: 0.894667\n",
      "[12/100] Loss: 0.387914, Acc: 0.895278\n",
      "Finish 12 epoch, Loss: 0.388821, Acc: 0.894983\n",
      "Test Loss: 0.368085, Acc: 0.902300\n",
      "Time:6.9 s\n",
      "\n",
      "**********\n",
      "epoch 13\n",
      "[13/100] Loss: 0.383241, Acc: 0.895729\n",
      "[13/100] Loss: 0.385270, Acc: 0.895781\n",
      "[13/100] Loss: 0.390466, Acc: 0.894826\n",
      "[13/100] Loss: 0.389110, Acc: 0.894557\n",
      "[13/100] Loss: 0.387176, Acc: 0.895125\n",
      "[13/100] Loss: 0.386116, Acc: 0.895260\n",
      "Finish 13 epoch, Loss: 0.386200, Acc: 0.895433\n",
      "Test Loss: 0.365634, Acc: 0.902600\n",
      "Time:7.0 s\n",
      "\n",
      "**********\n",
      "epoch 14\n",
      "[14/100] Loss: 0.383606, Acc: 0.898750\n",
      "[14/100] Loss: 0.383980, Acc: 0.896563\n",
      "[14/100] Loss: 0.383559, Acc: 0.897326\n",
      "[14/100] Loss: 0.384300, Acc: 0.897318\n",
      "[14/100] Loss: 0.383719, Acc: 0.896479\n",
      "[14/100] Loss: 0.383267, Acc: 0.896215\n",
      "Finish 14 epoch, Loss: 0.383681, Acc: 0.896167\n",
      "Test Loss: 0.363356, Acc: 0.902800\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 15\n",
      "[15/100] Loss: 0.381691, Acc: 0.896667\n",
      "[15/100] Loss: 0.386296, Acc: 0.895885\n",
      "[15/100] Loss: 0.386332, Acc: 0.896389\n",
      "[15/100] Loss: 0.383793, Acc: 0.896823\n",
      "[15/100] Loss: 0.380744, Acc: 0.897417\n",
      "[15/100] Loss: 0.379435, Acc: 0.897222\n",
      "Finish 15 epoch, Loss: 0.381294, Acc: 0.896850\n",
      "Test Loss: 0.361101, Acc: 0.903500\n",
      "Time:7.2 s\n",
      "\n",
      "**********\n",
      "epoch 16\n",
      "[16/100] Loss: 0.389023, Acc: 0.895312\n",
      "[16/100] Loss: 0.380432, Acc: 0.896302\n",
      "[16/100] Loss: 0.376589, Acc: 0.898715\n",
      "[16/100] Loss: 0.379672, Acc: 0.897474\n",
      "[16/100] Loss: 0.379209, Acc: 0.897083\n",
      "[16/100] Loss: 0.379299, Acc: 0.896823\n",
      "Finish 16 epoch, Loss: 0.379024, Acc: 0.896883\n",
      "Test Loss: 0.359085, Acc: 0.903600\n",
      "Time:7.2 s\n",
      "\n",
      "**********\n",
      "epoch 17\n",
      "[17/100] Loss: 0.379195, Acc: 0.896563\n",
      "[17/100] Loss: 0.377902, Acc: 0.897344\n",
      "[17/100] Loss: 0.377758, Acc: 0.897535\n",
      "[17/100] Loss: 0.378497, Acc: 0.897396\n",
      "[17/100] Loss: 0.378651, Acc: 0.896792\n",
      "[17/100] Loss: 0.377248, Acc: 0.897726\n",
      "Finish 17 epoch, Loss: 0.376848, Acc: 0.897700\n",
      "Test Loss: 0.357157, Acc: 0.903800\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 18\n",
      "[18/100] Loss: 0.371119, Acc: 0.898958\n",
      "[18/100] Loss: 0.368788, Acc: 0.898854\n",
      "[18/100] Loss: 0.375941, Acc: 0.896806\n",
      "[18/100] Loss: 0.376700, Acc: 0.897344\n",
      "[18/100] Loss: 0.374178, Acc: 0.898542\n",
      "[18/100] Loss: 0.373858, Acc: 0.898576\n",
      "Finish 18 epoch, Loss: 0.374794, Acc: 0.898217\n",
      "Test Loss: 0.355259, Acc: 0.904400\n",
      "Time:7.3 s\n",
      "\n",
      "**********\n",
      "epoch 19\n",
      "[19/100] Loss: 0.373672, Acc: 0.896354\n",
      "[19/100] Loss: 0.370947, Acc: 0.898594\n",
      "[19/100] Loss: 0.371961, Acc: 0.898924\n",
      "[19/100] Loss: 0.373535, Acc: 0.898464\n",
      "[19/100] Loss: 0.372124, Acc: 0.899167\n",
      "[19/100] Loss: 0.373191, Acc: 0.898611\n",
      "Finish 19 epoch, Loss: 0.372804, Acc: 0.898500\n",
      "Test Loss: 0.353516, Acc: 0.904400\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 20\n",
      "[20/100] Loss: 0.371396, Acc: 0.897917\n",
      "[20/100] Loss: 0.375447, Acc: 0.897240\n",
      "[20/100] Loss: 0.370703, Acc: 0.899097\n",
      "[20/100] Loss: 0.373121, Acc: 0.898724\n",
      "[20/100] Loss: 0.373258, Acc: 0.898312\n",
      "[20/100] Loss: 0.372270, Acc: 0.898941\n",
      "Finish 20 epoch, Loss: 0.370898, Acc: 0.899100\n",
      "Test Loss: 0.351683, Acc: 0.905100\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 21\n",
      "[21/100] Loss: 0.358947, Acc: 0.900833\n",
      "[21/100] Loss: 0.363043, Acc: 0.899740\n",
      "[21/100] Loss: 0.366006, Acc: 0.899896\n",
      "[21/100] Loss: 0.368470, Acc: 0.899792\n",
      "[21/100] Loss: 0.369740, Acc: 0.899312\n",
      "[21/100] Loss: 0.369619, Acc: 0.899340\n",
      "Finish 21 epoch, Loss: 0.369080, Acc: 0.899400\n",
      "Test Loss: 0.349993, Acc: 0.905000\n",
      "Time:6.4 s\n",
      "\n",
      "**********\n",
      "epoch 22\n",
      "[22/100] Loss: 0.358740, Acc: 0.903438\n",
      "[22/100] Loss: 0.356745, Acc: 0.903385\n",
      "[22/100] Loss: 0.361755, Acc: 0.901806\n",
      "[22/100] Loss: 0.367006, Acc: 0.900234\n",
      "[22/100] Loss: 0.369017, Acc: 0.899250\n",
      "[22/100] Loss: 0.367959, Acc: 0.899219\n",
      "Finish 22 epoch, Loss: 0.367341, Acc: 0.899817\n",
      "Test Loss: 0.348498, Acc: 0.905500\n",
      "Time:6.1 s\n",
      "\n",
      "**********\n",
      "epoch 23\n",
      "[23/100] Loss: 0.361786, Acc: 0.902083\n",
      "[23/100] Loss: 0.368881, Acc: 0.899687\n",
      "[23/100] Loss: 0.362171, Acc: 0.901111\n",
      "[23/100] Loss: 0.363750, Acc: 0.900599\n",
      "[23/100] Loss: 0.364624, Acc: 0.900417\n",
      "[23/100] Loss: 0.366263, Acc: 0.899844\n",
      "Finish 23 epoch, Loss: 0.365634, Acc: 0.900117\n",
      "Test Loss: 0.346912, Acc: 0.906000\n",
      "Time:6.0 s\n",
      "\n",
      "**********\n",
      "epoch 24\n",
      "[24/100] Loss: 0.365868, Acc: 0.897396\n",
      "[24/100] Loss: 0.367992, Acc: 0.898802\n",
      "[24/100] Loss: 0.366222, Acc: 0.899896\n",
      "[24/100] Loss: 0.365467, Acc: 0.900625\n",
      "[24/100] Loss: 0.367363, Acc: 0.899750\n",
      "[24/100] Loss: 0.364388, Acc: 0.900503\n",
      "Finish 24 epoch, Loss: 0.364029, Acc: 0.900517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.345629, Acc: 0.905900\n",
      "Time:6.0 s\n",
      "\n",
      "**********\n",
      "epoch 25\n",
      "[25/100] Loss: 0.354907, Acc: 0.900625\n",
      "[25/100] Loss: 0.361147, Acc: 0.900365\n",
      "[25/100] Loss: 0.361052, Acc: 0.900903\n",
      "[25/100] Loss: 0.363055, Acc: 0.900052\n",
      "[25/100] Loss: 0.362596, Acc: 0.900104\n",
      "[25/100] Loss: 0.363270, Acc: 0.900174\n",
      "Finish 25 epoch, Loss: 0.362472, Acc: 0.900700\n",
      "Test Loss: 0.344154, Acc: 0.906200\n",
      "Time:6.4 s\n",
      "\n",
      "**********\n",
      "epoch 26\n",
      "[26/100] Loss: 0.364088, Acc: 0.899479\n",
      "[26/100] Loss: 0.366480, Acc: 0.899844\n",
      "[26/100] Loss: 0.362293, Acc: 0.900486\n",
      "[26/100] Loss: 0.360572, Acc: 0.900859\n",
      "[26/100] Loss: 0.361293, Acc: 0.900875\n",
      "[26/100] Loss: 0.361350, Acc: 0.900868\n",
      "Finish 26 epoch, Loss: 0.360986, Acc: 0.900883\n",
      "Test Loss: 0.342806, Acc: 0.906300\n",
      "Time:6.2 s\n",
      "\n",
      "**********\n",
      "epoch 27\n",
      "[27/100] Loss: 0.368032, Acc: 0.900521\n",
      "[27/100] Loss: 0.361821, Acc: 0.902083\n",
      "[27/100] Loss: 0.358167, Acc: 0.902465\n",
      "[27/100] Loss: 0.357675, Acc: 0.902448\n",
      "[27/100] Loss: 0.358734, Acc: 0.901542\n",
      "[27/100] Loss: 0.360823, Acc: 0.900885\n",
      "Finish 27 epoch, Loss: 0.359530, Acc: 0.901233\n",
      "Test Loss: 0.341447, Acc: 0.906900\n",
      "Time:6.9 s\n",
      "\n",
      "**********\n",
      "epoch 28\n",
      "[28/100] Loss: 0.361891, Acc: 0.900937\n",
      "[28/100] Loss: 0.351676, Acc: 0.902917\n",
      "[28/100] Loss: 0.356212, Acc: 0.902222\n",
      "[28/100] Loss: 0.354616, Acc: 0.902682\n",
      "[28/100] Loss: 0.355013, Acc: 0.902792\n",
      "[28/100] Loss: 0.357908, Acc: 0.901997\n",
      "Finish 28 epoch, Loss: 0.358124, Acc: 0.901767\n",
      "Test Loss: 0.340194, Acc: 0.906900\n",
      "Time:7.8 s\n",
      "\n",
      "**********\n",
      "epoch 29\n",
      "[29/100] Loss: 0.360643, Acc: 0.901771\n",
      "[29/100] Loss: 0.356429, Acc: 0.901979\n",
      "[29/100] Loss: 0.356957, Acc: 0.902049\n",
      "[29/100] Loss: 0.356176, Acc: 0.902448\n",
      "[29/100] Loss: 0.358080, Acc: 0.901583\n",
      "[29/100] Loss: 0.355903, Acc: 0.902240\n",
      "Finish 29 epoch, Loss: 0.356777, Acc: 0.901833\n",
      "Test Loss: 0.338991, Acc: 0.907400\n",
      "Time:6.8 s\n",
      "\n",
      "**********\n",
      "epoch 30\n",
      "[30/100] Loss: 0.351074, Acc: 0.904583\n",
      "[30/100] Loss: 0.354765, Acc: 0.902344\n",
      "[30/100] Loss: 0.352131, Acc: 0.903021\n",
      "[30/100] Loss: 0.351777, Acc: 0.903203\n",
      "[30/100] Loss: 0.354080, Acc: 0.902062\n",
      "[30/100] Loss: 0.354353, Acc: 0.902014\n",
      "Finish 30 epoch, Loss: 0.355457, Acc: 0.901883\n",
      "Test Loss: 0.337841, Acc: 0.908300\n",
      "Time:6.9 s\n",
      "\n",
      "**********\n",
      "epoch 31\n",
      "[31/100] Loss: 0.347349, Acc: 0.905833\n",
      "[31/100] Loss: 0.345497, Acc: 0.904948\n",
      "[31/100] Loss: 0.348509, Acc: 0.903750\n",
      "[31/100] Loss: 0.349683, Acc: 0.903229\n",
      "[31/100] Loss: 0.353965, Acc: 0.901708\n",
      "[31/100] Loss: 0.354169, Acc: 0.902240\n",
      "Finish 31 epoch, Loss: 0.354192, Acc: 0.902200\n",
      "Test Loss: 0.336688, Acc: 0.908400\n",
      "Time:7.0 s\n",
      "\n",
      "**********\n",
      "epoch 32\n",
      "[32/100] Loss: 0.355119, Acc: 0.902292\n",
      "[32/100] Loss: 0.353686, Acc: 0.901979\n",
      "[32/100] Loss: 0.353098, Acc: 0.902361\n",
      "[32/100] Loss: 0.351180, Acc: 0.903932\n",
      "[32/100] Loss: 0.353982, Acc: 0.902750\n",
      "[32/100] Loss: 0.352976, Acc: 0.902604\n",
      "Finish 32 epoch, Loss: 0.352963, Acc: 0.902567\n",
      "Test Loss: 0.335745, Acc: 0.908700\n",
      "Time:7.2 s\n",
      "\n",
      "**********\n",
      "epoch 33\n",
      "[33/100] Loss: 0.351267, Acc: 0.902083\n",
      "[33/100] Loss: 0.352383, Acc: 0.902083\n",
      "[33/100] Loss: 0.352980, Acc: 0.903264\n",
      "[33/100] Loss: 0.352288, Acc: 0.903203\n",
      "[33/100] Loss: 0.352596, Acc: 0.902396\n",
      "[33/100] Loss: 0.351318, Acc: 0.903125\n",
      "Finish 33 epoch, Loss: 0.351773, Acc: 0.902867\n",
      "Test Loss: 0.334565, Acc: 0.908600\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 34\n",
      "[34/100] Loss: 0.344953, Acc: 0.906042\n",
      "[34/100] Loss: 0.346764, Acc: 0.904479\n",
      "[34/100] Loss: 0.349796, Acc: 0.904410\n",
      "[34/100] Loss: 0.350656, Acc: 0.903177\n",
      "[34/100] Loss: 0.351242, Acc: 0.903167\n",
      "[34/100] Loss: 0.350676, Acc: 0.903212\n",
      "Finish 34 epoch, Loss: 0.350613, Acc: 0.903050\n",
      "Test Loss: 0.333567, Acc: 0.908900\n",
      "Time:7.0 s\n",
      "\n",
      "**********\n",
      "epoch 35\n",
      "[35/100] Loss: 0.351732, Acc: 0.904792\n",
      "[35/100] Loss: 0.348960, Acc: 0.904844\n",
      "[35/100] Loss: 0.350604, Acc: 0.903611\n",
      "[35/100] Loss: 0.352464, Acc: 0.902813\n",
      "[35/100] Loss: 0.350890, Acc: 0.903333\n",
      "[35/100] Loss: 0.349706, Acc: 0.903594\n",
      "Finish 35 epoch, Loss: 0.349501, Acc: 0.903500\n",
      "Test Loss: 0.332552, Acc: 0.908800\n",
      "Time:7.6 s\n",
      "\n",
      "**********\n",
      "epoch 36\n",
      "[36/100] Loss: 0.351714, Acc: 0.905521\n",
      "[36/100] Loss: 0.352528, Acc: 0.904271\n",
      "[36/100] Loss: 0.348802, Acc: 0.904722\n",
      "[36/100] Loss: 0.347165, Acc: 0.905729\n",
      "[36/100] Loss: 0.349293, Acc: 0.904438\n",
      "[36/100] Loss: 0.348553, Acc: 0.903872\n",
      "Finish 36 epoch, Loss: 0.348406, Acc: 0.903800\n",
      "Test Loss: 0.331631, Acc: 0.909100\n",
      "Time:7.2 s\n",
      "\n",
      "**********\n",
      "epoch 37\n",
      "[37/100] Loss: 0.351714, Acc: 0.901250\n",
      "[37/100] Loss: 0.346993, Acc: 0.903073\n",
      "[37/100] Loss: 0.343150, Acc: 0.904410\n",
      "[37/100] Loss: 0.347186, Acc: 0.903307\n",
      "[37/100] Loss: 0.347514, Acc: 0.903417\n",
      "[37/100] Loss: 0.346749, Acc: 0.904028\n",
      "Finish 37 epoch, Loss: 0.347341, Acc: 0.903983\n",
      "Test Loss: 0.330700, Acc: 0.909400\n",
      "Time:6.8 s\n",
      "\n",
      "**********\n",
      "epoch 38\n",
      "[38/100] Loss: 0.343886, Acc: 0.903958\n",
      "[38/100] Loss: 0.343470, Acc: 0.905729\n",
      "[38/100] Loss: 0.345278, Acc: 0.904931\n",
      "[38/100] Loss: 0.347189, Acc: 0.903698\n",
      "[38/100] Loss: 0.348066, Acc: 0.903896\n",
      "[38/100] Loss: 0.346793, Acc: 0.904549\n",
      "Finish 38 epoch, Loss: 0.346329, Acc: 0.904317\n",
      "Test Loss: 0.329719, Acc: 0.909000\n",
      "Time:7.9 s\n",
      "\n",
      "**********\n",
      "epoch 39\n",
      "[39/100] Loss: 0.333656, Acc: 0.909583\n",
      "[39/100] Loss: 0.340052, Acc: 0.905833\n",
      "[39/100] Loss: 0.344885, Acc: 0.904028\n",
      "[39/100] Loss: 0.347561, Acc: 0.902839\n",
      "[39/100] Loss: 0.346386, Acc: 0.904229\n",
      "[39/100] Loss: 0.345419, Acc: 0.904635\n",
      "Finish 39 epoch, Loss: 0.345320, Acc: 0.904600\n",
      "Test Loss: 0.328787, Acc: 0.909400\n",
      "Time:6.2 s\n",
      "\n",
      "**********\n",
      "epoch 40\n",
      "[40/100] Loss: 0.344516, Acc: 0.908542\n",
      "[40/100] Loss: 0.348031, Acc: 0.903385\n",
      "[40/100] Loss: 0.348981, Acc: 0.903229\n",
      "[40/100] Loss: 0.345403, Acc: 0.904167\n",
      "[40/100] Loss: 0.345648, Acc: 0.903958\n",
      "[40/100] Loss: 0.344194, Acc: 0.904861\n",
      "Finish 40 epoch, Loss: 0.344356, Acc: 0.904833\n",
      "Test Loss: 0.328002, Acc: 0.909700\n",
      "Time:7.2 s\n",
      "\n",
      "**********\n",
      "epoch 41\n",
      "[41/100] Loss: 0.359743, Acc: 0.899167\n",
      "[41/100] Loss: 0.351706, Acc: 0.902760\n",
      "[41/100] Loss: 0.350453, Acc: 0.902569\n",
      "[41/100] Loss: 0.349420, Acc: 0.903047\n",
      "[41/100] Loss: 0.346877, Acc: 0.903917\n",
      "[41/100] Loss: 0.344703, Acc: 0.904583\n",
      "Finish 41 epoch, Loss: 0.343403, Acc: 0.905167\n",
      "Test Loss: 0.327223, Acc: 0.909500\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 42\n",
      "[42/100] Loss: 0.336592, Acc: 0.908229\n",
      "[42/100] Loss: 0.345206, Acc: 0.905937\n",
      "[42/100] Loss: 0.343115, Acc: 0.905625\n",
      "[42/100] Loss: 0.343047, Acc: 0.905417\n",
      "[42/100] Loss: 0.343836, Acc: 0.905271\n",
      "[42/100] Loss: 0.342571, Acc: 0.905434\n",
      "Finish 42 epoch, Loss: 0.342480, Acc: 0.905383\n",
      "Test Loss: 0.326438, Acc: 0.910200\n",
      "Time:6.4 s\n",
      "\n",
      "**********\n",
      "epoch 43\n",
      "[43/100] Loss: 0.322554, Acc: 0.910417\n",
      "[43/100] Loss: 0.334502, Acc: 0.907500\n",
      "[43/100] Loss: 0.340876, Acc: 0.905729\n",
      "[43/100] Loss: 0.342926, Acc: 0.905547\n",
      "[43/100] Loss: 0.342459, Acc: 0.905604\n",
      "[43/100] Loss: 0.341796, Acc: 0.905747\n",
      "Finish 43 epoch, Loss: 0.341566, Acc: 0.905750\n",
      "Test Loss: 0.325600, Acc: 0.910800\n",
      "Time:6.8 s\n",
      "\n",
      "**********\n",
      "epoch 44\n",
      "[44/100] Loss: 0.329450, Acc: 0.910417\n",
      "[44/100] Loss: 0.337104, Acc: 0.908333\n",
      "[44/100] Loss: 0.336736, Acc: 0.907396\n",
      "[44/100] Loss: 0.339160, Acc: 0.906927\n",
      "[44/100] Loss: 0.339430, Acc: 0.906750\n",
      "[44/100] Loss: 0.340525, Acc: 0.906337\n",
      "Finish 44 epoch, Loss: 0.340683, Acc: 0.906083\n",
      "Test Loss: 0.324928, Acc: 0.910300\n",
      "Time:6.4 s\n",
      "\n",
      "**********\n",
      "epoch 45\n",
      "[45/100] Loss: 0.344147, Acc: 0.901563\n",
      "[45/100] Loss: 0.343413, Acc: 0.903698\n",
      "[45/100] Loss: 0.340322, Acc: 0.905382\n",
      "[45/100] Loss: 0.341597, Acc: 0.905937\n",
      "[45/100] Loss: 0.343183, Acc: 0.904813\n",
      "[45/100] Loss: 0.339917, Acc: 0.906094\n",
      "Finish 45 epoch, Loss: 0.339827, Acc: 0.906050\n",
      "Test Loss: 0.324099, Acc: 0.910500\n",
      "Time:6.1 s\n",
      "\n",
      "**********\n",
      "epoch 46\n",
      "[46/100] Loss: 0.328729, Acc: 0.906667\n",
      "[46/100] Loss: 0.339380, Acc: 0.904479\n",
      "[46/100] Loss: 0.337397, Acc: 0.905069\n",
      "[46/100] Loss: 0.336562, Acc: 0.906432\n",
      "[46/100] Loss: 0.336596, Acc: 0.906604\n",
      "[46/100] Loss: 0.339214, Acc: 0.906163\n",
      "Finish 46 epoch, Loss: 0.338988, Acc: 0.906367\n",
      "Test Loss: 0.323259, Acc: 0.911300\n",
      "Time:6.1 s\n",
      "\n",
      "**********\n",
      "epoch 47\n",
      "[47/100] Loss: 0.353580, Acc: 0.897500\n",
      "[47/100] Loss: 0.342744, Acc: 0.903854\n",
      "[47/100] Loss: 0.341242, Acc: 0.904201\n",
      "[47/100] Loss: 0.339601, Acc: 0.905182\n",
      "[47/100] Loss: 0.339901, Acc: 0.905375\n",
      "[47/100] Loss: 0.339316, Acc: 0.906146\n",
      "Finish 47 epoch, Loss: 0.338164, Acc: 0.906567\n",
      "Test Loss: 0.322613, Acc: 0.911100\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 48\n",
      "[48/100] Loss: 0.332131, Acc: 0.907917\n",
      "[48/100] Loss: 0.341454, Acc: 0.905469\n",
      "[48/100] Loss: 0.339506, Acc: 0.907118\n",
      "[48/100] Loss: 0.337896, Acc: 0.907057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48/100] Loss: 0.337749, Acc: 0.907333\n",
      "[48/100] Loss: 0.337607, Acc: 0.906771\n",
      "Finish 48 epoch, Loss: 0.337363, Acc: 0.906733\n",
      "Test Loss: 0.321873, Acc: 0.911500\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 49\n",
      "[49/100] Loss: 0.319657, Acc: 0.914167\n",
      "[49/100] Loss: 0.327045, Acc: 0.911146\n",
      "[49/100] Loss: 0.332446, Acc: 0.908854\n",
      "[49/100] Loss: 0.332575, Acc: 0.908620\n",
      "[49/100] Loss: 0.333156, Acc: 0.907792\n",
      "[49/100] Loss: 0.336672, Acc: 0.906944\n",
      "Finish 49 epoch, Loss: 0.336579, Acc: 0.907000\n",
      "Test Loss: 0.321280, Acc: 0.911400\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 50\n",
      "[50/100] Loss: 0.333244, Acc: 0.905000\n",
      "[50/100] Loss: 0.337404, Acc: 0.906927\n",
      "[50/100] Loss: 0.336448, Acc: 0.906424\n",
      "[50/100] Loss: 0.332194, Acc: 0.907865\n",
      "[50/100] Loss: 0.331738, Acc: 0.907792\n",
      "[50/100] Loss: 0.335818, Acc: 0.906979\n",
      "Finish 50 epoch, Loss: 0.335831, Acc: 0.907050\n",
      "Test Loss: 0.320596, Acc: 0.912100\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 51\n",
      "[51/100] Loss: 0.337573, Acc: 0.904687\n",
      "[51/100] Loss: 0.339726, Acc: 0.905937\n",
      "[51/100] Loss: 0.335296, Acc: 0.908090\n",
      "[51/100] Loss: 0.333684, Acc: 0.907214\n",
      "[51/100] Loss: 0.335424, Acc: 0.906854\n",
      "[51/100] Loss: 0.335074, Acc: 0.907066\n",
      "Finish 51 epoch, Loss: 0.335079, Acc: 0.906983\n",
      "Test Loss: 0.319903, Acc: 0.912200\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 52\n",
      "[52/100] Loss: 0.333624, Acc: 0.906875\n",
      "[52/100] Loss: 0.337143, Acc: 0.907760\n",
      "[52/100] Loss: 0.334909, Acc: 0.906910\n",
      "[52/100] Loss: 0.334396, Acc: 0.907109\n",
      "[52/100] Loss: 0.333837, Acc: 0.907917\n",
      "[52/100] Loss: 0.333310, Acc: 0.907969\n",
      "Finish 52 epoch, Loss: 0.334355, Acc: 0.907333\n",
      "Test Loss: 0.319341, Acc: 0.912700\n",
      "Time:6.2 s\n",
      "\n",
      "**********\n",
      "epoch 53\n",
      "[53/100] Loss: 0.330091, Acc: 0.909896\n",
      "[53/100] Loss: 0.329121, Acc: 0.908542\n",
      "[53/100] Loss: 0.334841, Acc: 0.906493\n",
      "[53/100] Loss: 0.334536, Acc: 0.907057\n",
      "[53/100] Loss: 0.335329, Acc: 0.906687\n",
      "[53/100] Loss: 0.334255, Acc: 0.907292\n",
      "Finish 53 epoch, Loss: 0.333629, Acc: 0.907383\n",
      "Test Loss: 0.318812, Acc: 0.912800\n",
      "Time:6.0 s\n",
      "\n",
      "**********\n",
      "epoch 54\n",
      "[54/100] Loss: 0.329693, Acc: 0.908333\n",
      "[54/100] Loss: 0.326680, Acc: 0.909531\n",
      "[54/100] Loss: 0.329497, Acc: 0.908507\n",
      "[54/100] Loss: 0.332209, Acc: 0.907708\n",
      "[54/100] Loss: 0.332370, Acc: 0.907625\n",
      "[54/100] Loss: 0.333375, Acc: 0.907674\n",
      "Finish 54 epoch, Loss: 0.332927, Acc: 0.907783\n",
      "Test Loss: 0.318248, Acc: 0.913100\n",
      "Time:6.3 s\n",
      "\n",
      "**********\n",
      "epoch 55\n",
      "[55/100] Loss: 0.313032, Acc: 0.916146\n",
      "[55/100] Loss: 0.329831, Acc: 0.910625\n",
      "[55/100] Loss: 0.331397, Acc: 0.909444\n",
      "[55/100] Loss: 0.328935, Acc: 0.909271\n",
      "[55/100] Loss: 0.330747, Acc: 0.908667\n",
      "[55/100] Loss: 0.330853, Acc: 0.908333\n",
      "Finish 55 epoch, Loss: 0.332260, Acc: 0.908133\n",
      "Test Loss: 0.317567, Acc: 0.912900\n",
      "Time:6.1 s\n",
      "\n",
      "**********\n",
      "epoch 56\n",
      "[56/100] Loss: 0.332249, Acc: 0.909375\n",
      "[56/100] Loss: 0.327176, Acc: 0.909844\n",
      "[56/100] Loss: 0.324740, Acc: 0.910868\n",
      "[56/100] Loss: 0.328609, Acc: 0.910443\n",
      "[56/100] Loss: 0.330045, Acc: 0.909583\n",
      "[56/100] Loss: 0.332202, Acc: 0.908194\n",
      "Finish 56 epoch, Loss: 0.331570, Acc: 0.908267\n",
      "Test Loss: 0.316966, Acc: 0.913900\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 57\n",
      "[57/100] Loss: 0.338001, Acc: 0.906250\n",
      "[57/100] Loss: 0.336327, Acc: 0.906615\n",
      "[57/100] Loss: 0.335609, Acc: 0.906563\n",
      "[57/100] Loss: 0.331478, Acc: 0.907760\n",
      "[57/100] Loss: 0.333116, Acc: 0.907646\n",
      "[57/100] Loss: 0.332397, Acc: 0.908177\n",
      "Finish 57 epoch, Loss: 0.330921, Acc: 0.908433\n",
      "Test Loss: 0.316331, Acc: 0.913500\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 58\n",
      "[58/100] Loss: 0.320646, Acc: 0.910937\n",
      "[58/100] Loss: 0.325810, Acc: 0.909896\n",
      "[58/100] Loss: 0.329396, Acc: 0.908160\n",
      "[58/100] Loss: 0.330651, Acc: 0.907813\n",
      "[58/100] Loss: 0.328633, Acc: 0.908438\n",
      "[58/100] Loss: 0.328872, Acc: 0.908681\n",
      "Finish 58 epoch, Loss: 0.330277, Acc: 0.908250\n",
      "Test Loss: 0.315859, Acc: 0.913300\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 59\n",
      "[59/100] Loss: 0.327721, Acc: 0.907604\n",
      "[59/100] Loss: 0.331093, Acc: 0.907031\n",
      "[59/100] Loss: 0.328813, Acc: 0.907986\n",
      "[59/100] Loss: 0.328817, Acc: 0.909010\n",
      "[59/100] Loss: 0.328116, Acc: 0.909271\n",
      "[59/100] Loss: 0.328969, Acc: 0.909115\n",
      "Finish 59 epoch, Loss: 0.329648, Acc: 0.908750\n",
      "Test Loss: 0.315326, Acc: 0.914100\n",
      "Time:6.1 s\n",
      "\n",
      "**********\n",
      "epoch 60\n",
      "[60/100] Loss: 0.344403, Acc: 0.905000\n",
      "[60/100] Loss: 0.338053, Acc: 0.907969\n",
      "[60/100] Loss: 0.333446, Acc: 0.908681\n",
      "[60/100] Loss: 0.330070, Acc: 0.908932\n",
      "[60/100] Loss: 0.330772, Acc: 0.907937\n",
      "[60/100] Loss: 0.328912, Acc: 0.908767\n",
      "Finish 60 epoch, Loss: 0.329019, Acc: 0.908850\n",
      "Test Loss: 0.314889, Acc: 0.913900\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 61\n",
      "[61/100] Loss: 0.338212, Acc: 0.906875\n",
      "[61/100] Loss: 0.340448, Acc: 0.906406\n",
      "[61/100] Loss: 0.333915, Acc: 0.907951\n",
      "[61/100] Loss: 0.330003, Acc: 0.909141\n",
      "[61/100] Loss: 0.327624, Acc: 0.909604\n",
      "[61/100] Loss: 0.327187, Acc: 0.909566\n",
      "Finish 61 epoch, Loss: 0.328401, Acc: 0.909150\n",
      "Test Loss: 0.314185, Acc: 0.914300\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 62\n",
      "[62/100] Loss: 0.328972, Acc: 0.907813\n",
      "[62/100] Loss: 0.329670, Acc: 0.907396\n",
      "[62/100] Loss: 0.325003, Acc: 0.909479\n",
      "[62/100] Loss: 0.324360, Acc: 0.909948\n",
      "[62/100] Loss: 0.325981, Acc: 0.909937\n",
      "[62/100] Loss: 0.327274, Acc: 0.909323\n",
      "Finish 62 epoch, Loss: 0.327823, Acc: 0.909100\n",
      "Test Loss: 0.313700, Acc: 0.913800\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 63\n",
      "[63/100] Loss: 0.318220, Acc: 0.910833\n",
      "[63/100] Loss: 0.324765, Acc: 0.908646\n",
      "[63/100] Loss: 0.323320, Acc: 0.909931\n",
      "[63/100] Loss: 0.325653, Acc: 0.909557\n",
      "[63/100] Loss: 0.326399, Acc: 0.909833\n",
      "[63/100] Loss: 0.327817, Acc: 0.909549\n",
      "Finish 63 epoch, Loss: 0.327234, Acc: 0.909383\n",
      "Test Loss: 0.313168, Acc: 0.914200\n",
      "Time:6.3 s\n",
      "\n",
      "**********\n",
      "epoch 64\n",
      "[64/100] Loss: 0.330026, Acc: 0.907917\n",
      "[64/100] Loss: 0.328978, Acc: 0.909323\n",
      "[64/100] Loss: 0.330269, Acc: 0.908021\n",
      "[64/100] Loss: 0.329481, Acc: 0.908828\n",
      "[64/100] Loss: 0.327133, Acc: 0.910104\n",
      "[64/100] Loss: 0.326737, Acc: 0.909566\n",
      "Finish 64 epoch, Loss: 0.326665, Acc: 0.909433\n",
      "Test Loss: 0.312725, Acc: 0.914200\n",
      "Time:6.2 s\n",
      "\n",
      "**********\n",
      "epoch 65\n",
      "[65/100] Loss: 0.334138, Acc: 0.906563\n",
      "[65/100] Loss: 0.333642, Acc: 0.908906\n",
      "[65/100] Loss: 0.329153, Acc: 0.909757\n",
      "[65/100] Loss: 0.326619, Acc: 0.910365\n",
      "[65/100] Loss: 0.325764, Acc: 0.910146\n",
      "[65/100] Loss: 0.326514, Acc: 0.909306\n",
      "Finish 65 epoch, Loss: 0.326095, Acc: 0.909667\n",
      "Test Loss: 0.312290, Acc: 0.914500\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 66\n",
      "[66/100] Loss: 0.324760, Acc: 0.910417\n",
      "[66/100] Loss: 0.320755, Acc: 0.911875\n",
      "[66/100] Loss: 0.323577, Acc: 0.910417\n",
      "[66/100] Loss: 0.324420, Acc: 0.909844\n",
      "[66/100] Loss: 0.323987, Acc: 0.910125\n",
      "[66/100] Loss: 0.326102, Acc: 0.909549\n",
      "Finish 66 epoch, Loss: 0.325535, Acc: 0.909833\n",
      "Test Loss: 0.311836, Acc: 0.914400\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 67\n",
      "[67/100] Loss: 0.320156, Acc: 0.910000\n",
      "[67/100] Loss: 0.321029, Acc: 0.910260\n",
      "[67/100] Loss: 0.326570, Acc: 0.909167\n",
      "[67/100] Loss: 0.327836, Acc: 0.909062\n",
      "[67/100] Loss: 0.327701, Acc: 0.909229\n",
      "[67/100] Loss: 0.325948, Acc: 0.909583\n",
      "Finish 67 epoch, Loss: 0.324991, Acc: 0.909817\n",
      "Test Loss: 0.311384, Acc: 0.914600\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 68\n",
      "[68/100] Loss: 0.313795, Acc: 0.911771\n",
      "[68/100] Loss: 0.322111, Acc: 0.909219\n",
      "[68/100] Loss: 0.323783, Acc: 0.909549\n",
      "[68/100] Loss: 0.320994, Acc: 0.910990\n",
      "[68/100] Loss: 0.323425, Acc: 0.910104\n",
      "[68/100] Loss: 0.323405, Acc: 0.910365\n",
      "Finish 68 epoch, Loss: 0.324448, Acc: 0.910017\n",
      "Test Loss: 0.311004, Acc: 0.915000\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 69\n",
      "[69/100] Loss: 0.320099, Acc: 0.915417\n",
      "[69/100] Loss: 0.321523, Acc: 0.912813\n",
      "[69/100] Loss: 0.324267, Acc: 0.910694\n",
      "[69/100] Loss: 0.325355, Acc: 0.909896\n",
      "[69/100] Loss: 0.325903, Acc: 0.910021\n",
      "[69/100] Loss: 0.323904, Acc: 0.910382\n",
      "Finish 69 epoch, Loss: 0.323931, Acc: 0.910167\n",
      "Test Loss: 0.310517, Acc: 0.915200\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 70\n",
      "[70/100] Loss: 0.325154, Acc: 0.910521\n",
      "[70/100] Loss: 0.326608, Acc: 0.911250\n",
      "[70/100] Loss: 0.324351, Acc: 0.910139\n",
      "[70/100] Loss: 0.324955, Acc: 0.910104\n",
      "[70/100] Loss: 0.325003, Acc: 0.909750\n",
      "[70/100] Loss: 0.322460, Acc: 0.910625\n",
      "Finish 70 epoch, Loss: 0.323419, Acc: 0.910350\n",
      "Test Loss: 0.310007, Acc: 0.915400\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 71\n",
      "[71/100] Loss: 0.328514, Acc: 0.908542\n",
      "[71/100] Loss: 0.328507, Acc: 0.908073\n",
      "[71/100] Loss: 0.325234, Acc: 0.910521\n",
      "[71/100] Loss: 0.323460, Acc: 0.910964\n",
      "[71/100] Loss: 0.322549, Acc: 0.911563\n",
      "[71/100] Loss: 0.323093, Acc: 0.910590\n",
      "Finish 71 epoch, Loss: 0.322915, Acc: 0.910700\n",
      "Test Loss: 0.309630, Acc: 0.916000\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72/100] Loss: 0.322806, Acc: 0.908333\n",
      "[72/100] Loss: 0.317712, Acc: 0.910521\n",
      "[72/100] Loss: 0.317926, Acc: 0.910868\n",
      "[72/100] Loss: 0.320065, Acc: 0.910495\n",
      "[72/100] Loss: 0.321042, Acc: 0.910438\n",
      "[72/100] Loss: 0.322283, Acc: 0.910781\n",
      "Finish 72 epoch, Loss: 0.322404, Acc: 0.910667\n",
      "Test Loss: 0.309293, Acc: 0.915200\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 73\n",
      "[73/100] Loss: 0.303124, Acc: 0.915937\n",
      "[73/100] Loss: 0.310970, Acc: 0.912865\n",
      "[73/100] Loss: 0.314634, Acc: 0.912951\n",
      "[73/100] Loss: 0.316900, Acc: 0.911823\n",
      "[73/100] Loss: 0.319182, Acc: 0.910771\n",
      "[73/100] Loss: 0.322449, Acc: 0.910590\n",
      "Finish 73 epoch, Loss: 0.321926, Acc: 0.910733\n",
      "Test Loss: 0.308825, Acc: 0.915600\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 74\n",
      "[74/100] Loss: 0.310226, Acc: 0.911667\n",
      "[74/100] Loss: 0.312869, Acc: 0.913281\n",
      "[74/100] Loss: 0.315948, Acc: 0.912674\n",
      "[74/100] Loss: 0.321216, Acc: 0.911120\n",
      "[74/100] Loss: 0.320611, Acc: 0.911521\n",
      "[74/100] Loss: 0.320663, Acc: 0.911198\n",
      "Finish 74 epoch, Loss: 0.321435, Acc: 0.910850\n",
      "Test Loss: 0.308342, Acc: 0.915700\n",
      "Time:6.4 s\n",
      "\n",
      "**********\n",
      "epoch 75\n",
      "[75/100] Loss: 0.316852, Acc: 0.915312\n",
      "[75/100] Loss: 0.322259, Acc: 0.912969\n",
      "[75/100] Loss: 0.325853, Acc: 0.910729\n",
      "[75/100] Loss: 0.323241, Acc: 0.911224\n",
      "[75/100] Loss: 0.320584, Acc: 0.911188\n",
      "[75/100] Loss: 0.319552, Acc: 0.911424\n",
      "Finish 75 epoch, Loss: 0.320968, Acc: 0.911050\n",
      "Test Loss: 0.307994, Acc: 0.916000\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 76\n",
      "[76/100] Loss: 0.320307, Acc: 0.911979\n",
      "[76/100] Loss: 0.315326, Acc: 0.913281\n",
      "[76/100] Loss: 0.320734, Acc: 0.911910\n",
      "[76/100] Loss: 0.321503, Acc: 0.911016\n",
      "[76/100] Loss: 0.320766, Acc: 0.910792\n",
      "[76/100] Loss: 0.320748, Acc: 0.911146\n",
      "Finish 76 epoch, Loss: 0.320491, Acc: 0.911317\n",
      "Test Loss: 0.307631, Acc: 0.915700\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 77\n",
      "[77/100] Loss: 0.307984, Acc: 0.914062\n",
      "[77/100] Loss: 0.318349, Acc: 0.911719\n",
      "[77/100] Loss: 0.322557, Acc: 0.910243\n",
      "[77/100] Loss: 0.321722, Acc: 0.910807\n",
      "[77/100] Loss: 0.319937, Acc: 0.911396\n",
      "[77/100] Loss: 0.319890, Acc: 0.911319\n",
      "Finish 77 epoch, Loss: 0.320037, Acc: 0.911450\n",
      "Test Loss: 0.307277, Acc: 0.916200\n",
      "Time:6.8 s\n",
      "\n",
      "**********\n",
      "epoch 78\n",
      "[78/100] Loss: 0.318934, Acc: 0.911771\n",
      "[78/100] Loss: 0.321684, Acc: 0.909687\n",
      "[78/100] Loss: 0.317364, Acc: 0.911528\n",
      "[78/100] Loss: 0.316974, Acc: 0.911380\n",
      "[78/100] Loss: 0.318684, Acc: 0.911563\n",
      "[78/100] Loss: 0.319366, Acc: 0.911458\n",
      "Finish 78 epoch, Loss: 0.319581, Acc: 0.911350\n",
      "Test Loss: 0.306888, Acc: 0.916300\n",
      "Time:6.8 s\n",
      "\n",
      "**********\n",
      "epoch 79\n",
      "[79/100] Loss: 0.323111, Acc: 0.910312\n",
      "[79/100] Loss: 0.324234, Acc: 0.909375\n",
      "[79/100] Loss: 0.323064, Acc: 0.910382\n",
      "[79/100] Loss: 0.321704, Acc: 0.911016\n",
      "[79/100] Loss: 0.319564, Acc: 0.911771\n",
      "[79/100] Loss: 0.318571, Acc: 0.911753\n",
      "Finish 79 epoch, Loss: 0.319126, Acc: 0.911550\n",
      "Test Loss: 0.306565, Acc: 0.916200\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 80\n",
      "[80/100] Loss: 0.310892, Acc: 0.916562\n",
      "[80/100] Loss: 0.315830, Acc: 0.913698\n",
      "[80/100] Loss: 0.316736, Acc: 0.912778\n",
      "[80/100] Loss: 0.317524, Acc: 0.912135\n",
      "[80/100] Loss: 0.317596, Acc: 0.911854\n",
      "[80/100] Loss: 0.317804, Acc: 0.911962\n",
      "Finish 80 epoch, Loss: 0.318686, Acc: 0.911767\n",
      "Test Loss: 0.306270, Acc: 0.915800\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 81\n",
      "[81/100] Loss: 0.328042, Acc: 0.906146\n",
      "[81/100] Loss: 0.316895, Acc: 0.912656\n",
      "[81/100] Loss: 0.314406, Acc: 0.913542\n",
      "[81/100] Loss: 0.313126, Acc: 0.913672\n",
      "[81/100] Loss: 0.316624, Acc: 0.912646\n",
      "[81/100] Loss: 0.318508, Acc: 0.911649\n",
      "Finish 81 epoch, Loss: 0.318269, Acc: 0.911767\n",
      "Test Loss: 0.305782, Acc: 0.916500\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 82\n",
      "[82/100] Loss: 0.326724, Acc: 0.912396\n",
      "[82/100] Loss: 0.320314, Acc: 0.913229\n",
      "[82/100] Loss: 0.320631, Acc: 0.913750\n",
      "[82/100] Loss: 0.318703, Acc: 0.912813\n",
      "[82/100] Loss: 0.316077, Acc: 0.912271\n",
      "[82/100] Loss: 0.316804, Acc: 0.912396\n",
      "Finish 82 epoch, Loss: 0.317822, Acc: 0.912033\n",
      "Test Loss: 0.305375, Acc: 0.916400\n",
      "Time:6.4 s\n",
      "\n",
      "**********\n",
      "epoch 83\n",
      "[83/100] Loss: 0.308000, Acc: 0.915521\n",
      "[83/100] Loss: 0.313091, Acc: 0.914635\n",
      "[83/100] Loss: 0.314563, Acc: 0.913542\n",
      "[83/100] Loss: 0.314400, Acc: 0.913151\n",
      "[83/100] Loss: 0.315181, Acc: 0.912687\n",
      "[83/100] Loss: 0.316074, Acc: 0.912361\n",
      "Finish 83 epoch, Loss: 0.317418, Acc: 0.911733\n",
      "Test Loss: 0.305106, Acc: 0.916400\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 84\n",
      "[84/100] Loss: 0.319397, Acc: 0.911354\n",
      "[84/100] Loss: 0.320892, Acc: 0.909375\n",
      "[84/100] Loss: 0.318738, Acc: 0.910556\n",
      "[84/100] Loss: 0.317584, Acc: 0.911250\n",
      "[84/100] Loss: 0.315408, Acc: 0.912625\n",
      "[84/100] Loss: 0.317328, Acc: 0.912257\n",
      "Finish 84 epoch, Loss: 0.317001, Acc: 0.912233\n",
      "Test Loss: 0.304744, Acc: 0.916500\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 85\n",
      "[85/100] Loss: 0.310739, Acc: 0.915833\n",
      "[85/100] Loss: 0.315862, Acc: 0.913073\n",
      "[85/100] Loss: 0.320306, Acc: 0.911458\n",
      "[85/100] Loss: 0.316556, Acc: 0.912552\n",
      "[85/100] Loss: 0.317255, Acc: 0.912542\n",
      "[85/100] Loss: 0.316212, Acc: 0.912292\n",
      "Finish 85 epoch, Loss: 0.316591, Acc: 0.912017\n",
      "Test Loss: 0.304400, Acc: 0.916600\n",
      "Time:6.2 s\n",
      "\n",
      "**********\n",
      "epoch 86\n",
      "[86/100] Loss: 0.320986, Acc: 0.913646\n",
      "[86/100] Loss: 0.316540, Acc: 0.913333\n",
      "[86/100] Loss: 0.315196, Acc: 0.913403\n",
      "[86/100] Loss: 0.314487, Acc: 0.913229\n",
      "[86/100] Loss: 0.314237, Acc: 0.913458\n",
      "[86/100] Loss: 0.315131, Acc: 0.912795\n",
      "Finish 86 epoch, Loss: 0.316176, Acc: 0.912383\n",
      "Test Loss: 0.304089, Acc: 0.916900\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 87\n",
      "[87/100] Loss: 0.325052, Acc: 0.909479\n",
      "[87/100] Loss: 0.316532, Acc: 0.912500\n",
      "[87/100] Loss: 0.316895, Acc: 0.911944\n",
      "[87/100] Loss: 0.319085, Acc: 0.910703\n",
      "[87/100] Loss: 0.317219, Acc: 0.912208\n",
      "[87/100] Loss: 0.316250, Acc: 0.912483\n",
      "Finish 87 epoch, Loss: 0.315798, Acc: 0.912467\n",
      "Test Loss: 0.303761, Acc: 0.916700\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 88\n",
      "[88/100] Loss: 0.317658, Acc: 0.914167\n",
      "[88/100] Loss: 0.313981, Acc: 0.912917\n",
      "[88/100] Loss: 0.309665, Acc: 0.914201\n",
      "[88/100] Loss: 0.312348, Acc: 0.913828\n",
      "[88/100] Loss: 0.315489, Acc: 0.912021\n",
      "[88/100] Loss: 0.315543, Acc: 0.912257\n",
      "Finish 88 epoch, Loss: 0.315396, Acc: 0.912550\n",
      "Test Loss: 0.303534, Acc: 0.917000\n",
      "Time:6.9 s\n",
      "\n",
      "**********\n",
      "epoch 89\n",
      "[89/100] Loss: 0.320362, Acc: 0.911979\n",
      "[89/100] Loss: 0.320623, Acc: 0.910104\n",
      "[89/100] Loss: 0.317300, Acc: 0.911701\n",
      "[89/100] Loss: 0.313813, Acc: 0.913021\n",
      "[89/100] Loss: 0.313140, Acc: 0.912854\n",
      "[89/100] Loss: 0.314083, Acc: 0.912865\n",
      "Finish 89 epoch, Loss: 0.315006, Acc: 0.912667\n",
      "Test Loss: 0.303093, Acc: 0.917200\n",
      "Time:6.0 s\n",
      "\n",
      "**********\n",
      "epoch 90\n",
      "[90/100] Loss: 0.315730, Acc: 0.910625\n",
      "[90/100] Loss: 0.318638, Acc: 0.911667\n",
      "[90/100] Loss: 0.320999, Acc: 0.910312\n",
      "[90/100] Loss: 0.321052, Acc: 0.911146\n",
      "[90/100] Loss: 0.315749, Acc: 0.912333\n",
      "[90/100] Loss: 0.315733, Acc: 0.912413\n",
      "Finish 90 epoch, Loss: 0.314632, Acc: 0.912633\n",
      "Test Loss: 0.302816, Acc: 0.917400\n",
      "Time:6.4 s\n",
      "\n",
      "**********\n",
      "epoch 91\n",
      "[91/100] Loss: 0.301837, Acc: 0.916042\n",
      "[91/100] Loss: 0.306379, Acc: 0.915052\n",
      "[91/100] Loss: 0.311948, Acc: 0.913090\n",
      "[91/100] Loss: 0.312553, Acc: 0.912813\n",
      "[91/100] Loss: 0.313243, Acc: 0.913021\n",
      "[91/100] Loss: 0.313402, Acc: 0.913247\n",
      "Finish 91 epoch, Loss: 0.314257, Acc: 0.912750\n",
      "Test Loss: 0.302561, Acc: 0.917300\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 92\n",
      "[92/100] Loss: 0.343947, Acc: 0.901667\n",
      "[92/100] Loss: 0.321160, Acc: 0.909531\n",
      "[92/100] Loss: 0.318419, Acc: 0.910694\n",
      "[92/100] Loss: 0.313969, Acc: 0.911979\n",
      "[92/100] Loss: 0.317445, Acc: 0.911208\n",
      "[92/100] Loss: 0.314618, Acc: 0.912813\n",
      "Finish 92 epoch, Loss: 0.313894, Acc: 0.912883\n",
      "Test Loss: 0.302275, Acc: 0.917000\n",
      "Time:6.8 s\n",
      "\n",
      "**********\n",
      "epoch 93\n",
      "[93/100] Loss: 0.319426, Acc: 0.910104\n",
      "[93/100] Loss: 0.314973, Acc: 0.912396\n",
      "[93/100] Loss: 0.314736, Acc: 0.912917\n",
      "[93/100] Loss: 0.314407, Acc: 0.912656\n",
      "[93/100] Loss: 0.315209, Acc: 0.912458\n",
      "[93/100] Loss: 0.313781, Acc: 0.912951\n",
      "Finish 93 epoch, Loss: 0.313524, Acc: 0.912883\n",
      "Test Loss: 0.301938, Acc: 0.917200\n",
      "Time:6.3 s\n",
      "\n",
      "**********\n",
      "epoch 94\n",
      "[94/100] Loss: 0.316514, Acc: 0.910729\n",
      "[94/100] Loss: 0.318037, Acc: 0.911510\n",
      "[94/100] Loss: 0.318837, Acc: 0.911042\n",
      "[94/100] Loss: 0.313046, Acc: 0.912891\n",
      "[94/100] Loss: 0.313292, Acc: 0.912792\n",
      "[94/100] Loss: 0.314038, Acc: 0.912951\n",
      "Finish 94 epoch, Loss: 0.313167, Acc: 0.913133\n",
      "Test Loss: 0.301684, Acc: 0.917500\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 95\n",
      "[95/100] Loss: 0.312491, Acc: 0.912500\n",
      "[95/100] Loss: 0.308360, Acc: 0.913229\n",
      "[95/100] Loss: 0.313763, Acc: 0.911875\n",
      "[95/100] Loss: 0.312988, Acc: 0.912240\n",
      "[95/100] Loss: 0.313256, Acc: 0.912729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95/100] Loss: 0.312316, Acc: 0.913247\n",
      "Finish 95 epoch, Loss: 0.312796, Acc: 0.913150\n",
      "Test Loss: 0.301313, Acc: 0.917200\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 96\n",
      "[96/100] Loss: 0.308951, Acc: 0.913958\n",
      "[96/100] Loss: 0.311510, Acc: 0.914375\n",
      "[96/100] Loss: 0.312829, Acc: 0.913472\n",
      "[96/100] Loss: 0.312264, Acc: 0.913516\n",
      "[96/100] Loss: 0.312081, Acc: 0.913687\n",
      "[96/100] Loss: 0.312873, Acc: 0.913351\n",
      "Finish 96 epoch, Loss: 0.312468, Acc: 0.913283\n",
      "Test Loss: 0.301021, Acc: 0.917400\n",
      "Time:6.8 s\n",
      "\n",
      "**********\n",
      "epoch 97\n",
      "[97/100] Loss: 0.327282, Acc: 0.908646\n",
      "[97/100] Loss: 0.319713, Acc: 0.910208\n",
      "[97/100] Loss: 0.314261, Acc: 0.911458\n",
      "[97/100] Loss: 0.313370, Acc: 0.912656\n",
      "[97/100] Loss: 0.315176, Acc: 0.912625\n",
      "[97/100] Loss: 0.312379, Acc: 0.913264\n",
      "Finish 97 epoch, Loss: 0.312121, Acc: 0.913383\n",
      "Test Loss: 0.300767, Acc: 0.917400\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 98\n",
      "[98/100] Loss: 0.314655, Acc: 0.912500\n",
      "[98/100] Loss: 0.315580, Acc: 0.912031\n",
      "[98/100] Loss: 0.312513, Acc: 0.912847\n",
      "[98/100] Loss: 0.311076, Acc: 0.913255\n",
      "[98/100] Loss: 0.311883, Acc: 0.913438\n",
      "[98/100] Loss: 0.312083, Acc: 0.913073\n",
      "Finish 98 epoch, Loss: 0.311771, Acc: 0.913333\n",
      "Test Loss: 0.300509, Acc: 0.917700\n",
      "Time:7.9 s\n",
      "\n",
      "**********\n",
      "epoch 99\n",
      "[99/100] Loss: 0.304894, Acc: 0.914375\n",
      "[99/100] Loss: 0.309397, Acc: 0.913906\n",
      "[99/100] Loss: 0.307629, Acc: 0.914271\n",
      "[99/100] Loss: 0.307835, Acc: 0.914375\n",
      "[99/100] Loss: 0.309703, Acc: 0.913937\n",
      "[99/100] Loss: 0.311566, Acc: 0.913438\n",
      "Finish 99 epoch, Loss: 0.311430, Acc: 0.913367\n",
      "Test Loss: 0.300275, Acc: 0.917700\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 100\n",
      "[100/100] Loss: 0.310755, Acc: 0.913125\n",
      "[100/100] Loss: 0.315450, Acc: 0.912031\n",
      "[100/100] Loss: 0.313673, Acc: 0.912708\n",
      "[100/100] Loss: 0.312385, Acc: 0.914010\n",
      "[100/100] Loss: 0.312286, Acc: 0.914000\n",
      "[100/100] Loss: 0.310860, Acc: 0.913663\n",
      "Finish 100 epoch, Loss: 0.311103, Acc: 0.913550\n",
      "Test Loss: 0.299973, Acc: 0.917400\n",
      "Time:6.8 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    print('*' * 10)\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    since = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)  # 将图片展开成 28x28\n",
    "        if torch.cuda.is_available():\n",
    "            img = Variable(img).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "        else:\n",
    "            img = Variable(img)\n",
    "            label = Variable(label)\n",
    "        # 向前传播\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss.data[0] * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        running_acc += num_correct.data[0]\n",
    "        # 向后传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 300 == 0:\n",
    "            print('[{}/{}] Loss: {:.6f}, Acc: {:.6f}'.format(\n",
    "                epoch + 1, num_epochs, running_loss / (batch_size * i),\n",
    "                running_acc / (batch_size * i)))\n",
    "    \n",
    "    print('Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(\n",
    "        epoch + 1, running_loss / (len(train_dataset)), running_acc / (len(\n",
    "            train_dataset))))\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        if torch.cuda.is_available():\n",
    "            img = Variable(img, volatile=True).cuda()\n",
    "            label = Variable(label, volatile=True).cuda()\n",
    "        else:\n",
    "            img = Variable(img, volatile=True)\n",
    "            label = Variable(label, volatile=True)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss.data[0] * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        eval_acc += num_correct.data[0]\n",
    "    print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "        test_dataset)), eval_acc / (len(test_dataset))))\n",
    "    print('Time:{:.1f} s'.format(time.time() - since))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), './logstic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
