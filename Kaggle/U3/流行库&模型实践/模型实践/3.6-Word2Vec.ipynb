{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.词向量(Word2vec)技术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用20类新闻文本(20newsgroups)进行词向量训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从sklearn.datasets导入20类新闻文本抓取器\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# 通过互联网即时下载数据\n",
    "news = fetch_20newsgroups(subset='all')\n",
    "X, y = news.data, news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从bs4导入Beautifulsoup\n",
    "from bs4 import BeautifulSoup\n",
    "# 导入nltk和re工具包\n",
    "import nltk, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义一个函数名为news_to_sentences将每条新闻中的句子逐一剥离出来，并返回一个句子的列表\n",
    "def news_to_sentences(news):\n",
    "    news_text = BeautifulSoup(news).get_text()\n",
    "    tokenizer = nltk.data.load('nltk:tokenizer/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(news_text)\n",
    "    sentences = []\n",
    "    for sent in raw_sentences:\n",
    "        sentences.append(re.sub('[^a-zA-Z]', ' ', sent.lower().strip()).split())\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file D:\\Program Soft\\Anaconda\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# 将长篇新闻文本中的句子剥离出来用于训练\n",
    "sentences = []\n",
    "for x in X:\n",
    "    sentences += news_to_sentences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Soft\\Anaconda\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 从gensim.models里导入word2vec\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 配置词向量的维度\n",
    "num_features = 300\n",
    "# 保证被考虑的词汇的维度\n",
    "min_word_count = 20\n",
    "# 设定并行化训练使用CPU计算核心的数量，多核可用\n",
    "num_workers = 2\n",
    "# 定义训练词向量的上下文窗口大小\n",
    "context = 5\n",
    "downsampling = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练词向量模型\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                          workers=num_workers,\n",
    "                          size=num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 这个设定代表当前训练好的词向量模型为最终版，也可以加快模型的训练速度\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('afternoon', 0.7968169450759888),\n",
       " ('weekend', 0.7668416500091553),\n",
       " ('evening', 0.7512824535369873),\n",
       " ('saturday', 0.7300482988357544),\n",
       " ('night', 0.7086350917816162),\n",
       " ('friday', 0.6921929121017456),\n",
       " ('newspaper', 0.6695731282234192),\n",
       " ('summer', 0.6637552976608276),\n",
       " ('sunday', 0.6545504331588745),\n",
       " ('week', 0.6449211835861206)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用训练好的模型，寻找训练文本中与morning最相关的10个词汇\n",
    "model.most_similar('morning')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
